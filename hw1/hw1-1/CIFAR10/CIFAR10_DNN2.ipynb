{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from keras.models import Model, Sequential\n",
    "from keras.optimizers import SGD, Adadelta\n",
    "from keras.layers import Input, Flatten, Dense, Dropout\n",
    "from keras.utils import np_utils\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.decomposition import PCA\n",
    "import keras\n",
    "from keras import backend as K\n",
    "\n",
    "\n",
    "cifar10 = tf.keras.datasets.cifar10\n",
    "epoch_range = 500"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compressed_weights(model):\n",
    "    model_weights = np.array(model.get_weights())\n",
    "    #print(DNN_weights)\n",
    "    comp = []\n",
    "    for i in range(len(model_weights)):\n",
    "        model_weights[i] = model_weights[i].reshape(len(model_weights[i]),-1)\n",
    "        model_weights[i] = model_weights[i].flatten()\n",
    "        model_weights[i] = model_weights[i].reshape(len(model_weights[i]),-1)\n",
    "        #print(model_weights[i].shape)\n",
    "        for j in range(len(model_weights[i])):\n",
    "            comp.append(model_weights[i][j])\n",
    "    comp = np.array(comp).flatten()\n",
    "    return comp\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_gradients_norm(model, inputs, outputs):\n",
    "    #from https://stackoverflow.com/questions/51140950/how-to-obtain-the-gradients-in-keras\n",
    "    \"\"\" Gets gradient of model for given inputs and outputs for all weights\"\"\"\n",
    "    grads = model.optimizer.get_gradients(model.total_loss, model.trainable_weights)\n",
    "    symb_inputs = (model._feed_inputs + model._feed_targets + model._feed_sample_weights)\n",
    "    f = K.function(symb_inputs, grads)\n",
    "    x, y, sample_weight = model._standardize_user_data(inputs, outputs)\n",
    "    output_grad = f(x + y + sample_weight)\n",
    "    grad_sum = 0\n",
    "    for i in range(len(output_grad)):\n",
    "        grad_sum += np.sum(output_grad[i])**2\n",
    "    grad_norm = grad_sum ** 0.5\n",
    "    return grad_norm\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MY_DNN2(object):\n",
    "    def __init__(self, width = 32, height = 32, channels = 3):\n",
    "        self.width = width\n",
    "        self.height = height\n",
    "        self.channels = channels\n",
    "        \n",
    "        self.shape = (self.width, self.height, self.channels)\n",
    "        self.optimizer = SGD(lr=0.01, momentum=0.9, decay=1e-8, nesterov=False)\n",
    "        \n",
    "        self.model = self.__model()\n",
    "        self.model.compile(loss='categorical_crossentropy', optimizer=self.optimizer, metrics=['accuracy'])\n",
    "        \n",
    "    def __model(self):\n",
    "        inputs = Input(shape = self.shape)\n",
    "        x = Flatten(input_shape = self.shape)(inputs)\n",
    "        x = Dense((self.width * self.height * self.channels), activation='relu')(x)\n",
    "        x = Dense(400)(x)\n",
    "        x = Dense(850)(x)\n",
    "        y = Dropout(0.2)(x)\n",
    "        outputs = Dense(10,activation='softmax')(y)\n",
    "        model = Model(inputs=inputs, outputs=outputs)\n",
    "        model.summary()\n",
    "        \n",
    "        return model\n",
    "        \n",
    "    def train(self, x_train, y_train, epochs = 500, batch = 256, collect_interval = 3):\n",
    "        loss = []\n",
    "        accuracy = []\n",
    "        weights = []\n",
    "        grads = []\n",
    "        \n",
    "        for cnt in range(epochs):\n",
    "            history = self.model.fit(x_train, y_train, batch_size=batch, verbose = 0)\n",
    "            print(\"epoch:%d ,loss:%s, accuracy:%s \"%(cnt, history.history['loss'], history.history['acc']))\n",
    "            loss.append(history.history['loss'])\n",
    "            accuracy.append(history.history['acc'])\n",
    "            grads.append(get_gradients_norm(self.model, x_train, y_train))\n",
    "            \n",
    "            if(cnt%collect_interval == 0):\n",
    "                w = compressed_weights(self.model)\n",
    "                weights.append(w)\n",
    "\n",
    "            \n",
    "        self.loss = loss\n",
    "        self.accuracy = accuracy\n",
    "        self.weights = weights\n",
    "        self.grads = grads\n",
    "    \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPU found\n"
     ]
    }
   ],
   "source": [
    "if __name__ == '__main__':\n",
    "    if tf.test.gpu_device_name():\n",
    "        print('GPU found')\n",
    "    else:\n",
    "        print(\"No GPU found\")\n",
    "    \n",
    "    (x_train, y_train),(x_test, y_test) = cifar10.load_data()\n",
    "    #normalization for not getting to big output for 'relu' and other activation\n",
    "    x_train = x_train/255\n",
    "    x_test = x_test/255\n",
    "    y_train = np_utils.to_categorical(y_train, num_classes=10)\n",
    "    y_test = np_utils.to_categorical(y_test, num_classes=10)\n",
    " \n",
    "    #for dense layer, the parameters = (input_shape + 1) * size;\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_1 (InputLayer)         (None, 32, 32, 3)         0         \n",
      "_________________________________________________________________\n",
      "flatten_1 (Flatten)          (None, 3072)              0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 3072)              9440256   \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 400)               1229200   \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 850)               340850    \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 850)               0         \n",
      "_________________________________________________________________\n",
      "dense_4 (Dense)              (None, 10)                8510      \n",
      "=================================================================\n",
      "Total params: 11,018,816\n",
      "Trainable params: 11,018,816\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "epoch:0 ,loss:[1.8278070425415038], accuracy:[0.3516999999809265] \n",
      "epoch:1 ,loss:[1.6246574169158936], accuracy:[0.42618000001907347] \n",
      "epoch:2 ,loss:[1.5639555853652953], accuracy:[0.44434] \n",
      "epoch:3 ,loss:[1.5033290059661866], accuracy:[0.46622000000953673] \n",
      "epoch:4 ,loss:[1.4651908708190917], accuracy:[0.48001999998092654] \n",
      "epoch:5 ,loss:[1.447451548500061], accuracy:[0.48582] \n",
      "epoch:6 ,loss:[1.4248816162109375], accuracy:[0.4961800000190735] \n",
      "epoch:7 ,loss:[1.4172303955841064], accuracy:[0.4963000000190735] \n",
      "epoch:8 ,loss:[1.3995261585617065], accuracy:[0.50528] \n",
      "epoch:9 ,loss:[1.3807389693450929], accuracy:[0.5084800000190735] \n",
      "epoch:10 ,loss:[1.3653828220367432], accuracy:[0.5133600000190734] \n",
      "epoch:11 ,loss:[1.343599025001526], accuracy:[0.5220000000190735] \n",
      "epoch:12 ,loss:[1.3179167949676514], accuracy:[0.532560000038147] \n",
      "epoch:13 ,loss:[1.3088676574325562], accuracy:[0.53404] \n",
      "epoch:14 ,loss:[1.2833489743804931], accuracy:[0.5453600000190735] \n",
      "epoch:15 ,loss:[1.2727887552261352], accuracy:[0.5488] \n",
      "epoch:16 ,loss:[1.249034793624878], accuracy:[0.5562] \n",
      "epoch:17 ,loss:[1.2421070125961304], accuracy:[0.5600399999809265] \n",
      "epoch:18 ,loss:[1.225765118637085], accuracy:[0.5653399999809265] \n",
      "epoch:19 ,loss:[1.2105600564193726], accuracy:[0.56872] \n",
      "epoch:20 ,loss:[1.2062203579330444], accuracy:[0.5703200000190735] \n",
      "epoch:21 ,loss:[1.1808925138092041], accuracy:[0.5806399999809265] \n",
      "epoch:22 ,loss:[1.177935241127014], accuracy:[0.5788799999809265] \n",
      "epoch:23 ,loss:[1.1715531555175782], accuracy:[0.583620000038147] \n",
      "epoch:24 ,loss:[1.1450407611846924], accuracy:[0.592979999961853] \n",
      "epoch:25 ,loss:[1.137064733428955], accuracy:[0.593000000038147] \n",
      "epoch:26 ,loss:[1.1221272624206542], accuracy:[0.5994] \n",
      "epoch:27 ,loss:[1.117789025039673], accuracy:[0.6031199999618531] \n",
      "epoch:28 ,loss:[1.0958490201568603], accuracy:[0.609520000038147] \n",
      "epoch:29 ,loss:[1.0724837696456908], accuracy:[0.6200999999809265] \n",
      "epoch:30 ,loss:[1.0675358056259154], accuracy:[0.61934] \n",
      "epoch:31 ,loss:[1.0342044511795043], accuracy:[0.63232] \n",
      "epoch:32 ,loss:[1.0235223526382446], accuracy:[0.6366800000190734] \n",
      "epoch:33 ,loss:[1.0117321359252929], accuracy:[0.6389000000190734] \n",
      "epoch:34 ,loss:[1.018843380241394], accuracy:[0.6360000000190735] \n",
      "epoch:35 ,loss:[0.9884355562973023], accuracy:[0.6469600000190735] \n",
      "epoch:36 ,loss:[0.9892019015693665], accuracy:[0.6469199999809265] \n",
      "epoch:37 ,loss:[0.9803363129806518], accuracy:[0.65054] \n",
      "epoch:38 ,loss:[0.9588339567184448], accuracy:[0.656619999961853] \n",
      "epoch:39 ,loss:[0.9457847811508179], accuracy:[0.6630599999809265] \n",
      "epoch:40 ,loss:[0.9333148021697998], accuracy:[0.6677599999809265] \n",
      "epoch:41 ,loss:[0.9217911562728882], accuracy:[0.670480000038147] \n",
      "epoch:42 ,loss:[0.9168372947502136], accuracy:[0.672660000038147] \n",
      "epoch:43 ,loss:[0.8964167245864868], accuracy:[0.680800000038147] \n",
      "epoch:44 ,loss:[0.8966898628234863], accuracy:[0.67884] \n",
      "epoch:45 ,loss:[0.8678913290214538], accuracy:[0.6911799999809265] \n",
      "epoch:46 ,loss:[0.8548699507141113], accuracy:[0.6943999999809265] \n",
      "epoch:47 ,loss:[0.8504551954269409], accuracy:[0.6989800000190735] \n",
      "epoch:48 ,loss:[0.8402082612800598], accuracy:[0.69914] \n",
      "epoch:49 ,loss:[0.8350553549575805], accuracy:[0.7011200000190735] \n",
      "epoch:50 ,loss:[0.8134475802993775], accuracy:[0.70858] \n",
      "epoch:51 ,loss:[0.8114882400512695], accuracy:[0.708560000038147] \n",
      "epoch:52 ,loss:[0.7946339877319336], accuracy:[0.7143200000190735] \n",
      "epoch:53 ,loss:[0.7829861325073242], accuracy:[0.7203399999809265] \n",
      "epoch:54 ,loss:[0.7904804252243042], accuracy:[0.7173799999809265] \n",
      "epoch:55 ,loss:[0.7629540505599975], accuracy:[0.72838] \n",
      "epoch:56 ,loss:[0.7430810862541198], accuracy:[0.7353] \n",
      "epoch:57 ,loss:[0.7432315254402161], accuracy:[0.735720000038147] \n",
      "epoch:58 ,loss:[0.7368797596931458], accuracy:[0.73776] \n",
      "epoch:59 ,loss:[0.7369025039672852], accuracy:[0.7343400000381469] \n",
      "epoch:60 ,loss:[0.7272502199935913], accuracy:[0.739659999961853] \n",
      "epoch:61 ,loss:[0.726739131450653], accuracy:[0.73868] \n",
      "epoch:62 ,loss:[0.71990034491539], accuracy:[0.7408000000381469] \n",
      "epoch:63 ,loss:[0.6938791668510437], accuracy:[0.7537599999809265] \n",
      "epoch:64 ,loss:[0.7083505991363526], accuracy:[0.7446199999809265] \n",
      "epoch:65 ,loss:[0.6801864672088623], accuracy:[0.7575] \n",
      "epoch:66 ,loss:[0.6683632910823822], accuracy:[0.760899999961853] \n",
      "epoch:67 ,loss:[0.641966052494049], accuracy:[0.77162] \n",
      "epoch:68 ,loss:[0.6410470575904846], accuracy:[0.772840000038147] \n",
      "epoch:69 ,loss:[0.6382795514297486], accuracy:[0.77254] \n",
      "epoch:70 ,loss:[0.6488002795028687], accuracy:[0.76826] \n",
      "epoch:71 ,loss:[0.6150318354797363], accuracy:[0.78012] \n",
      "epoch:72 ,loss:[0.6077662754249573], accuracy:[0.781560000038147] \n",
      "epoch:73 ,loss:[0.5842272189331055], accuracy:[0.7929599999809265] \n",
      "epoch:74 ,loss:[0.5782050453186035], accuracy:[0.7932599999809266] \n",
      "epoch:75 ,loss:[0.5730311282157898], accuracy:[0.7949399999809265] \n",
      "epoch:76 ,loss:[0.5727930524826049], accuracy:[0.7956999999809266] \n",
      "epoch:77 ,loss:[0.5707228852272034], accuracy:[0.796999999961853] \n",
      "epoch:78 ,loss:[0.5643639142417908], accuracy:[0.7968199999809266] \n",
      "epoch:79 ,loss:[0.5454617308425903], accuracy:[0.8060600000381469] \n",
      "epoch:80 ,loss:[0.5417401779174804], accuracy:[0.8050999999809265] \n",
      "epoch:81 ,loss:[0.5410615883541107], accuracy:[0.8055599999809265] \n",
      "epoch:82 ,loss:[0.5373593264770508], accuracy:[0.808460000038147] \n",
      "epoch:83 ,loss:[0.521610393075943], accuracy:[0.813020000038147] \n",
      "epoch:84 ,loss:[0.5193626625061035], accuracy:[0.8143800000190735] \n",
      "epoch:85 ,loss:[0.5090325127220153], accuracy:[0.8189000000190735] \n",
      "epoch:86 ,loss:[0.5088744451522828], accuracy:[0.8180399999809265] \n",
      "epoch:87 ,loss:[0.5170077091503144], accuracy:[0.8164] \n",
      "epoch:88 ,loss:[0.48259206806182864], accuracy:[0.82862] \n",
      "epoch:89 ,loss:[0.47558052534103396], accuracy:[0.8311399999809265] \n",
      "epoch:90 ,loss:[0.4921138911628723], accuracy:[0.82426] \n",
      "epoch:91 ,loss:[0.5032821663665772], accuracy:[0.81998] \n",
      "epoch:92 ,loss:[0.4668879372215271], accuracy:[0.8326000000381469] \n",
      "epoch:93 ,loss:[0.4569743052864075], accuracy:[0.836979999961853] \n",
      "epoch:94 ,loss:[0.44570234897613525], accuracy:[0.84066] \n",
      "epoch:95 ,loss:[0.4365413306427002], accuracy:[0.8454800000381469] \n",
      "epoch:96 ,loss:[0.46256580468177794], accuracy:[0.8359600000190734] \n",
      "epoch:97 ,loss:[0.4375010842323303], accuracy:[0.84468] \n",
      "epoch:98 ,loss:[0.4445362600898743], accuracy:[0.84044] \n",
      "epoch:99 ,loss:[0.4306055988311768], accuracy:[0.8465799999618531] \n",
      "epoch:100 ,loss:[0.4249499667167664], accuracy:[0.8457799999809266] \n",
      "epoch:101 ,loss:[0.40449878770828246], accuracy:[0.8553199999809266] \n",
      "epoch:102 ,loss:[0.41233996435165404], accuracy:[0.8524200000190735] \n",
      "epoch:103 ,loss:[0.40713891907691957], accuracy:[0.8545800000190735] \n",
      "epoch:104 ,loss:[0.4010821965694428], accuracy:[0.855199999961853] \n",
      "epoch:105 ,loss:[0.39133362504959107], accuracy:[0.8600200000381469] \n",
      "epoch:106 ,loss:[0.40113470942497254], accuracy:[0.856259999961853] \n",
      "epoch:107 ,loss:[0.3958869284629822], accuracy:[0.8577200000190734] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:108 ,loss:[0.37724988344192506], accuracy:[0.8665599999618531] \n",
      "epoch:109 ,loss:[0.3723158095550537], accuracy:[0.8658] \n",
      "epoch:110 ,loss:[0.37793200775146485], accuracy:[0.86384] \n",
      "epoch:111 ,loss:[0.36297957553863525], accuracy:[0.871640000038147] \n",
      "epoch:112 ,loss:[0.35806741819381716], accuracy:[0.872040000038147] \n",
      "epoch:113 ,loss:[0.36297060800552366], accuracy:[0.8700200000190735] \n",
      "epoch:114 ,loss:[0.3416621067094803], accuracy:[0.87822] \n",
      "epoch:115 ,loss:[0.34088800621032717], accuracy:[0.877499999961853] \n",
      "epoch:116 ,loss:[0.33500503138542176], accuracy:[0.882079999961853] \n",
      "epoch:117 ,loss:[0.326155328540802], accuracy:[0.8843199999618531] \n",
      "epoch:118 ,loss:[0.36787842977523805], accuracy:[0.8684599999618531] \n",
      "epoch:119 ,loss:[0.3265287825059891], accuracy:[0.885420000038147] \n",
      "epoch:120 ,loss:[0.3153807840538025], accuracy:[0.8870400000190735] \n",
      "epoch:121 ,loss:[0.3121308500289917], accuracy:[0.887839999961853] \n",
      "epoch:122 ,loss:[0.32527321440696716], accuracy:[0.884920000038147] \n",
      "epoch:123 ,loss:[0.2995543051719666], accuracy:[0.892739999961853] \n",
      "epoch:124 ,loss:[0.3341110607910156], accuracy:[0.881139999961853] \n",
      "epoch:125 ,loss:[0.3173319615507126], accuracy:[0.8869999999618531] \n",
      "epoch:126 ,loss:[0.2744273908805847], accuracy:[0.902279999961853] \n",
      "epoch:127 ,loss:[0.3074933291053772], accuracy:[0.8901600000190735] \n",
      "epoch:128 ,loss:[0.3077126661682129], accuracy:[0.891600000038147] \n",
      "epoch:129 ,loss:[0.26809635416030886], accuracy:[0.9037400000190735] \n",
      "epoch:130 ,loss:[0.26955324341773984], accuracy:[0.9043199999809265] \n",
      "epoch:131 ,loss:[0.29258806829452516], accuracy:[0.89586] \n",
      "epoch:132 ,loss:[0.3006060880756378], accuracy:[0.8935399999618531] \n",
      "epoch:133 ,loss:[0.2874232883167267], accuracy:[0.8995400000190735] \n",
      "epoch:134 ,loss:[0.2595762861442566], accuracy:[0.907360000038147] \n",
      "epoch:135 ,loss:[0.2608720625591278], accuracy:[0.908239999961853] \n",
      "epoch:136 ,loss:[0.2746108909654617], accuracy:[0.9017799999809265] \n",
      "epoch:137 ,loss:[0.2754866806793213], accuracy:[0.90058] \n",
      "epoch:138 ,loss:[0.28386648054122926], accuracy:[0.8994000000190735] \n",
      "epoch:139 ,loss:[0.2575498426151276], accuracy:[0.90704] \n",
      "epoch:140 ,loss:[0.2578594448852539], accuracy:[0.9077000000190735] \n",
      "epoch:141 ,loss:[0.23321188179969787], accuracy:[0.917020000038147] \n",
      "epoch:142 ,loss:[0.24258216743946076], accuracy:[0.91376] \n",
      "epoch:143 ,loss:[0.2467104972076416], accuracy:[0.912479999961853] \n",
      "epoch:144 ,loss:[0.24536953988075255], accuracy:[0.9143999999809265] \n",
      "epoch:145 ,loss:[0.2640155226278305], accuracy:[0.9060000000190734] \n",
      "epoch:146 ,loss:[0.229261465883255], accuracy:[0.91944] \n",
      "epoch:147 ,loss:[0.24331014839172363], accuracy:[0.9144200000381469] \n",
      "epoch:148 ,loss:[0.23679348883628845], accuracy:[0.9154000000381469] \n",
      "epoch:149 ,loss:[0.23969659292221068], accuracy:[0.91566] \n",
      "epoch:150 ,loss:[0.2334339437007904], accuracy:[0.9174199999809265] \n",
      "epoch:151 ,loss:[0.2521134843349457], accuracy:[0.9096800000190735] \n",
      "epoch:152 ,loss:[0.23142562874317169], accuracy:[0.9191] \n",
      "epoch:153 ,loss:[0.23614909066200257], accuracy:[0.917560000038147] \n",
      "epoch:154 ,loss:[0.20867201088428497], accuracy:[0.9266000000190735] \n",
      "epoch:155 ,loss:[0.2042965148973465], accuracy:[0.928580000038147] \n",
      "epoch:156 ,loss:[0.2186137533378601], accuracy:[0.9232600000190735] \n",
      "epoch:157 ,loss:[0.2178199692249298], accuracy:[0.9223600000190735] \n",
      "epoch:158 ,loss:[0.20631196177482605], accuracy:[0.92712] \n",
      "epoch:159 ,loss:[0.22079794159412383], accuracy:[0.92192] \n",
      "epoch:160 ,loss:[0.21739115984916688], accuracy:[0.92286] \n",
      "epoch:161 ,loss:[0.19347727488994598], accuracy:[0.9309400000381469] \n",
      "epoch:162 ,loss:[0.23645293375015258], accuracy:[0.91642] \n",
      "epoch:163 ,loss:[0.21182771695137023], accuracy:[0.9255] \n",
      "epoch:164 ,loss:[0.19773472620010377], accuracy:[0.9299399999809265] \n",
      "epoch:165 ,loss:[0.18958026307106018], accuracy:[0.9311800000381469] \n",
      "epoch:166 ,loss:[0.2051356080055237], accuracy:[0.927539999961853] \n",
      "epoch:167 ,loss:[0.2167327313041687], accuracy:[0.9232800000190735] \n",
      "epoch:168 ,loss:[0.21259619691848755], accuracy:[0.925419999961853] \n",
      "epoch:169 ,loss:[0.17773158767700195], accuracy:[0.9365000000190735] \n",
      "epoch:170 ,loss:[0.20207377151966094], accuracy:[0.9288600000190735] \n",
      "epoch:171 ,loss:[0.18021314898490906], accuracy:[0.935039999961853] \n",
      "epoch:172 ,loss:[0.19430482498645782], accuracy:[0.9315199999809265] \n",
      "epoch:173 ,loss:[0.21493150451660156], accuracy:[0.9251800000381469] \n",
      "epoch:174 ,loss:[0.1678406123685837], accuracy:[0.9401599999618531] \n",
      "epoch:175 ,loss:[0.1615432729291916], accuracy:[0.9432200000190735] \n",
      "epoch:176 ,loss:[0.2106115874004364], accuracy:[0.924339999961853] \n",
      "epoch:177 ,loss:[0.19883693199157715], accuracy:[0.930780000038147] \n",
      "epoch:178 ,loss:[0.15750705513000487], accuracy:[0.945240000038147] \n",
      "epoch:179 ,loss:[0.13936730724334717], accuracy:[0.9513800000381469] \n",
      "epoch:180 ,loss:[0.17566508025169372], accuracy:[0.9379199999809266] \n",
      "epoch:181 ,loss:[0.22824938425064087], accuracy:[0.91988] \n",
      "epoch:182 ,loss:[0.17498901482105256], accuracy:[0.93886] \n",
      "epoch:183 ,loss:[0.14143098766326903], accuracy:[0.9495199999809265] \n",
      "epoch:184 ,loss:[0.16529652629375458], accuracy:[0.9422600000190735] \n",
      "epoch:185 ,loss:[0.16222883904457092], accuracy:[0.942199999961853] \n",
      "epoch:186 ,loss:[0.16030729198932647], accuracy:[0.9431599999809265] \n",
      "epoch:187 ,loss:[0.1420672474694252], accuracy:[0.9493799999618531] \n",
      "epoch:188 ,loss:[0.174767373752594], accuracy:[0.9384799999809265] \n",
      "epoch:189 ,loss:[0.22686579032421111], accuracy:[0.9206000000190735] \n",
      "epoch:190 ,loss:[0.1774615096259117], accuracy:[0.9380599999809265] \n",
      "epoch:191 ,loss:[0.14581108528614045], accuracy:[0.948359999961853] \n",
      "epoch:192 ,loss:[0.13495535856723787], accuracy:[0.9531399999809265] \n",
      "epoch:193 ,loss:[0.14197124064922334], accuracy:[0.950879999961853] \n",
      "epoch:194 ,loss:[0.1550992196559906], accuracy:[0.9460400000190735] \n",
      "epoch:195 ,loss:[0.19524888013839722], accuracy:[0.93196] \n",
      "epoch:196 ,loss:[0.1672106281709671], accuracy:[0.942459999961853] \n",
      "epoch:197 ,loss:[0.1391553026342392], accuracy:[0.95178] \n",
      "epoch:198 ,loss:[0.16040043867111206], accuracy:[0.94374] \n",
      "epoch:199 ,loss:[0.1376973749947548], accuracy:[0.952420000038147] \n",
      "epoch:200 ,loss:[0.1221914828324318], accuracy:[0.957080000038147] \n",
      "epoch:201 ,loss:[0.13107227787971495], accuracy:[0.953079999961853] \n",
      "epoch:202 ,loss:[0.1653378990149498], accuracy:[0.942339999961853] \n",
      "epoch:203 ,loss:[0.16037734447717666], accuracy:[0.9447799999809265] \n",
      "epoch:204 ,loss:[0.12488048913002014], accuracy:[0.9558199999809265] \n",
      "epoch:205 ,loss:[0.12655090611457825], accuracy:[0.9552599999618531] \n",
      "epoch:206 ,loss:[0.1772837443447113], accuracy:[0.9383599999809266] \n",
      "epoch:207 ,loss:[0.18510274279594421], accuracy:[0.935640000038147] \n",
      "epoch:208 ,loss:[0.130601289883852], accuracy:[0.953400000038147] \n",
      "epoch:209 ,loss:[0.10718970564842224], accuracy:[0.96276] \n",
      "epoch:210 ,loss:[0.11446943179130555], accuracy:[0.95986] \n",
      "epoch:211 ,loss:[0.10725336163043976], accuracy:[0.9621399999809265] \n",
      "epoch:212 ,loss:[0.11815285396575928], accuracy:[0.9581400000190735] \n",
      "epoch:213 ,loss:[0.1368433989048004], accuracy:[0.952059999961853] \n",
      "epoch:214 ,loss:[0.13945775707006455], accuracy:[0.950760000038147] \n",
      "epoch:215 ,loss:[0.10484840459346771], accuracy:[0.96312] \n",
      "epoch:216 ,loss:[0.15981058276176452], accuracy:[0.94424] \n",
      "epoch:217 ,loss:[0.2012666909790039], accuracy:[0.9314400000190735] \n",
      "epoch:218 ,loss:[0.1406849815130234], accuracy:[0.950459999961853] \n",
      "epoch:219 ,loss:[0.10481508640527726], accuracy:[0.9637799999809266] \n",
      "epoch:220 ,loss:[0.14085807249307633], accuracy:[0.950639999961853] \n",
      "epoch:221 ,loss:[0.13708665806770326], accuracy:[0.9528999999809266] \n",
      "epoch:222 ,loss:[0.10536185128450394], accuracy:[0.962300000038147] \n",
      "epoch:223 ,loss:[0.10517265476226807], accuracy:[0.9628000000381469] \n",
      "epoch:224 ,loss:[0.10155671772003173], accuracy:[0.9656799999809265] \n",
      "epoch:225 ,loss:[0.12090571182489396], accuracy:[0.9574199999809265] \n",
      "epoch:226 ,loss:[0.09388297784090043], accuracy:[0.967900000038147] \n",
      "epoch:227 ,loss:[0.08773833492040634], accuracy:[0.969840000038147] \n",
      "epoch:228 ,loss:[0.10827775575637817], accuracy:[0.961739999961853] \n",
      "epoch:229 ,loss:[0.10140663459777832], accuracy:[0.9652999999809265] \n",
      "epoch:230 ,loss:[0.11159488552093506], accuracy:[0.96088] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:231 ,loss:[0.16348108173370363], accuracy:[0.9443400000190735] \n",
      "epoch:232 ,loss:[0.15502792932748793], accuracy:[0.947139999961853] \n",
      "epoch:233 ,loss:[0.11847008631229401], accuracy:[0.959119999961853] \n",
      "epoch:234 ,loss:[0.13676101685523986], accuracy:[0.9526600000190735] \n",
      "epoch:235 ,loss:[0.15593149328231812], accuracy:[0.94652] \n",
      "epoch:236 ,loss:[0.14418148513317108], accuracy:[0.9508000000190735] \n",
      "epoch:237 ,loss:[0.11069013022661209], accuracy:[0.962520000038147] \n",
      "epoch:238 ,loss:[0.09874463539123535], accuracy:[0.965420000038147] \n",
      "epoch:239 ,loss:[0.07170668168783188], accuracy:[0.976000000038147] \n",
      "epoch:240 ,loss:[0.06238532018899918], accuracy:[0.978719999961853] \n",
      "epoch:241 ,loss:[0.08344531219959259], accuracy:[0.9710400000381469] \n",
      "epoch:242 ,loss:[0.09617150526523591], accuracy:[0.9672399999618531] \n",
      "epoch:243 ,loss:[0.10650537006855011], accuracy:[0.9635199999809265] \n",
      "epoch:244 ,loss:[0.09850445291996002], accuracy:[0.966119999961853] \n",
      "epoch:245 ,loss:[0.12300169862031937], accuracy:[0.9575000000190735] \n",
      "epoch:246 ,loss:[0.12581541505098343], accuracy:[0.956980000038147] \n",
      "epoch:247 ,loss:[0.08951384907960892], accuracy:[0.9685200000190735] \n",
      "epoch:248 ,loss:[0.07616911250352859], accuracy:[0.9732800000190734] \n",
      "epoch:249 ,loss:[0.06380869220256806], accuracy:[0.97826] \n",
      "epoch:250 ,loss:[0.07281601012706757], accuracy:[0.9751400000381469] \n",
      "epoch:251 ,loss:[0.08704973091363907], accuracy:[0.9700599999809265] \n",
      "epoch:252 ,loss:[0.15919161691665648], accuracy:[0.946840000038147] \n",
      "epoch:253 ,loss:[0.2635010177755356], accuracy:[0.9146000000190735] \n",
      "epoch:254 ,loss:[0.14478281144618987], accuracy:[0.950679999961853] \n",
      "epoch:255 ,loss:[0.07277664508104324], accuracy:[0.9758000000381469] \n",
      "epoch:256 ,loss:[0.05372067949771881], accuracy:[0.981940000038147] \n",
      "epoch:257 ,loss:[0.06317020810842514], accuracy:[0.97884] \n",
      "epoch:258 ,loss:[0.07381024559020996], accuracy:[0.974519999961853] \n",
      "epoch:259 ,loss:[0.05697085420131683], accuracy:[0.980319999961853] \n",
      "epoch:260 ,loss:[0.09547933324813843], accuracy:[0.9669200000381469] \n",
      "epoch:261 ,loss:[0.13224428590774537], accuracy:[0.9540999999809265] \n",
      "epoch:262 ,loss:[0.15147880415916443], accuracy:[0.948859999961853] \n",
      "epoch:263 ,loss:[0.12640567863464355], accuracy:[0.9559399999618531] \n",
      "epoch:264 ,loss:[0.09923037988185883], accuracy:[0.9664599999618531] \n",
      "epoch:265 ,loss:[0.07113470220565796], accuracy:[0.9763599999618531] \n",
      "epoch:266 ,loss:[0.09918576229810715], accuracy:[0.96656] \n",
      "epoch:267 ,loss:[0.07901670803308487], accuracy:[0.972319999961853] \n",
      "epoch:268 ,loss:[0.08282934071063995], accuracy:[0.971499999961853] \n",
      "epoch:269 ,loss:[0.07941489501476288], accuracy:[0.9739199999618531] \n",
      "epoch:270 ,loss:[0.10252307971954346], accuracy:[0.9651600000190735] \n",
      "epoch:271 ,loss:[0.11453202593803406], accuracy:[0.9602599999809265] \n",
      "epoch:272 ,loss:[0.11696554594993591], accuracy:[0.9607399999809265] \n",
      "epoch:273 ,loss:[0.15688101181030273], accuracy:[0.9476200000190735] \n",
      "epoch:274 ,loss:[0.12094320425510406], accuracy:[0.959440000038147] \n",
      "epoch:275 ,loss:[0.07181791584730149], accuracy:[0.9750200000190735] \n",
      "epoch:276 ,loss:[0.04934127201557159], accuracy:[0.9838600000190735] \n",
      "epoch:277 ,loss:[0.052996569061279294], accuracy:[0.982320000038147] \n",
      "epoch:278 ,loss:[0.04311343266963959], accuracy:[0.9854599999618531] \n",
      "epoch:279 ,loss:[0.04830162352502346], accuracy:[0.9836] \n",
      "epoch:280 ,loss:[0.04533761112689972], accuracy:[0.9848999999618531] \n",
      "epoch:281 ,loss:[0.08742655199527741], accuracy:[0.9710399999809265] \n",
      "epoch:282 ,loss:[0.19965590381622314], accuracy:[0.9355999999809265] \n",
      "epoch:283 ,loss:[0.18285801168441773], accuracy:[0.940419999961853] \n",
      "epoch:284 ,loss:[0.09501684367895126], accuracy:[0.96718] \n",
      "epoch:285 ,loss:[0.04946845225393772], accuracy:[0.98364] \n",
      "epoch:286 ,loss:[0.037517500965595245], accuracy:[0.988280000038147] \n",
      "epoch:287 ,loss:[0.03965443541526795], accuracy:[0.9872] \n",
      "epoch:288 ,loss:[0.034013891240358356], accuracy:[0.9895200000190735] \n",
      "epoch:289 ,loss:[0.0378054699164629], accuracy:[0.98778] \n",
      "epoch:290 ,loss:[0.04995193768978119], accuracy:[0.983139999961853] \n",
      "epoch:291 ,loss:[0.09313921601295472], accuracy:[0.96756] \n",
      "epoch:292 ,loss:[0.1237737265443802], accuracy:[0.958460000038147] \n",
      "epoch:293 ,loss:[0.11614859939336777], accuracy:[0.960980000038147] \n",
      "epoch:294 ,loss:[0.11537039845466614], accuracy:[0.9612599999618531] \n",
      "epoch:295 ,loss:[0.08773079535752534], accuracy:[0.97126] \n",
      "epoch:296 ,loss:[0.06505870152533054], accuracy:[0.9782800000190734] \n",
      "epoch:297 ,loss:[0.05935125494003296], accuracy:[0.980919999961853] \n",
      "epoch:298 ,loss:[0.07336487171411514], accuracy:[0.975979999961853] \n",
      "epoch:299 ,loss:[0.10199912958860398], accuracy:[0.9656999999618531] \n",
      "epoch:300 ,loss:[0.08809150895297527], accuracy:[0.9698400000190734] \n",
      "epoch:301 ,loss:[0.05301852005958557], accuracy:[0.9819799999809266] \n",
      "epoch:302 ,loss:[0.07867913103818894], accuracy:[0.97308] \n",
      "epoch:303 ,loss:[0.0633222894024849], accuracy:[0.9784000000190735] \n",
      "epoch:304 ,loss:[0.0695283260512352], accuracy:[0.9771399999618531] \n",
      "epoch:305 ,loss:[0.06616583756566048], accuracy:[0.9773600000190735] \n",
      "epoch:306 ,loss:[0.0384424184346199], accuracy:[0.9872000000190735] \n",
      "epoch:307 ,loss:[0.027465268718004227], accuracy:[0.9912400000190735] \n",
      "epoch:308 ,loss:[0.08648735935211181], accuracy:[0.971900000038147] \n",
      "epoch:309 ,loss:[0.211213302526474], accuracy:[0.93418] \n",
      "epoch:310 ,loss:[0.10875365792155266], accuracy:[0.9630400000381469] \n",
      "epoch:311 ,loss:[0.04964633734941482], accuracy:[0.9834800000190735] \n",
      "epoch:312 ,loss:[0.041802113955914974], accuracy:[0.9866] \n",
      "epoch:313 ,loss:[0.03795407059192658], accuracy:[0.9876000000190734] \n",
      "epoch:314 ,loss:[0.032804363638162616], accuracy:[0.98954] \n",
      "epoch:315 ,loss:[0.03666130236029625], accuracy:[0.9888200000190734] \n",
      "epoch:316 ,loss:[0.03223426320314407], accuracy:[0.988719999961853] \n",
      "epoch:317 ,loss:[0.047828434917926786], accuracy:[0.98384] \n",
      "epoch:318 ,loss:[0.06451316828608512], accuracy:[0.97866] \n",
      "epoch:319 ,loss:[0.11286919147491455], accuracy:[0.9632200000190735] \n",
      "epoch:320 ,loss:[0.14450266212463378], accuracy:[0.9533399999809266] \n",
      "epoch:321 ,loss:[0.14278110908985137], accuracy:[0.954519999961853] \n",
      "epoch:322 ,loss:[0.08077501935601235], accuracy:[0.973180000038147] \n",
      "epoch:323 ,loss:[0.055158273091316226], accuracy:[0.9815399999809266] \n",
      "epoch:324 ,loss:[0.06003765840768814], accuracy:[0.980079999961853] \n",
      "epoch:325 ,loss:[0.04500491992712021], accuracy:[0.9850600000190735] \n",
      "epoch:326 ,loss:[0.02767327060341835], accuracy:[0.99116] \n",
      "epoch:327 ,loss:[0.038993538773059847], accuracy:[0.9874200000190735] \n",
      "epoch:328 ,loss:[0.03264055744051933], accuracy:[0.9893600000190735] \n",
      "epoch:329 ,loss:[0.07574115410327911], accuracy:[0.9754200000190735] \n",
      "epoch:330 ,loss:[0.19323020970344543], accuracy:[0.938659999961853] \n",
      "epoch:331 ,loss:[0.11033588436603546], accuracy:[0.9633200000190735] \n",
      "epoch:332 ,loss:[0.0566689594244957], accuracy:[0.981259999961853] \n",
      "epoch:333 ,loss:[0.03884840283215046], accuracy:[0.98776] \n",
      "epoch:334 ,loss:[0.030101416783332825], accuracy:[0.99044] \n",
      "epoch:335 ,loss:[0.02319528559923172], accuracy:[0.9928600000190735] \n",
      "epoch:336 ,loss:[0.025328529800176622], accuracy:[0.9921] \n",
      "epoch:337 ,loss:[0.034180130842030046], accuracy:[0.9887] \n",
      "epoch:338 ,loss:[0.04005774554133415], accuracy:[0.9861200000190735] \n",
      "epoch:339 ,loss:[0.035270382087230684], accuracy:[0.9882] \n",
      "epoch:340 ,loss:[0.033551396782696245], accuracy:[0.98828] \n",
      "epoch:341 ,loss:[0.06027399190783501], accuracy:[0.9801000000190735] \n",
      "epoch:342 ,loss:[0.08787734837293625], accuracy:[0.9700400000190735] \n",
      "epoch:343 ,loss:[0.1542980883359909], accuracy:[0.9512599999809265] \n",
      "epoch:344 ,loss:[0.12781639750480653], accuracy:[0.958059999961853] \n",
      "epoch:345 ,loss:[0.07542240779042243], accuracy:[0.9749800000190735] \n",
      "epoch:346 ,loss:[0.06077763097882271], accuracy:[0.979860000038147] \n",
      "epoch:347 ,loss:[0.06156120771110058], accuracy:[0.97994] \n",
      "epoch:348 ,loss:[0.041902411836981776], accuracy:[0.9859] \n",
      "epoch:349 ,loss:[0.024585287071466445], accuracy:[0.992160000038147] \n",
      "epoch:350 ,loss:[0.021798046686649322], accuracy:[0.9940800000190735] \n",
      "epoch:351 ,loss:[0.042072753796577454], accuracy:[0.9860000000381469] \n",
      "epoch:352 ,loss:[0.14690002081394196], accuracy:[0.9538400000190735] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:353 ,loss:[0.14042901287555695], accuracy:[0.955220000038147] \n",
      "epoch:354 ,loss:[0.06082072392731905], accuracy:[0.98006] \n",
      "epoch:355 ,loss:[0.026650959069728853], accuracy:[0.9917600000190735] \n",
      "epoch:356 ,loss:[0.042322254079580304], accuracy:[0.98672] \n",
      "epoch:357 ,loss:[0.05353583365082741], accuracy:[0.9828000000190735] \n",
      "epoch:358 ,loss:[0.04423123780786991], accuracy:[0.9850800000190735] \n",
      "epoch:359 ,loss:[0.026548242664337158], accuracy:[0.991600000038147] \n",
      "epoch:360 ,loss:[0.017597671059668064], accuracy:[0.99492] \n",
      "epoch:361 ,loss:[0.01646732238829136], accuracy:[0.9952] \n",
      "epoch:362 ,loss:[0.020568441494852305], accuracy:[0.99374] \n",
      "epoch:363 ,loss:[0.08595641554117203], accuracy:[0.971999999961853] \n",
      "epoch:364 ,loss:[0.1452360127210617], accuracy:[0.952659999961853] \n",
      "epoch:365 ,loss:[0.10266850490748883], accuracy:[0.96524] \n",
      "epoch:366 ,loss:[0.05522408226132393], accuracy:[0.9811199999618531] \n",
      "epoch:367 ,loss:[0.03747712484002114], accuracy:[0.987860000038147] \n",
      "epoch:368 ,loss:[0.03219042294979096], accuracy:[0.98976] \n",
      "epoch:369 ,loss:[0.022123358982130886], accuracy:[0.99324] \n",
      "epoch:370 ,loss:[0.01532047108411789], accuracy:[0.995160000038147] \n",
      "epoch:371 ,loss:[0.03399641985654831], accuracy:[0.9892600000190734] \n",
      "epoch:372 ,loss:[0.0361866653072834], accuracy:[0.9875000000190735] \n",
      "epoch:373 ,loss:[0.0706459766292572], accuracy:[0.978260000038147] \n",
      "epoch:374 ,loss:[0.1127062053310871], accuracy:[0.963720000038147] \n",
      "epoch:375 ,loss:[0.07767371730089187], accuracy:[0.9746200000190735] \n",
      "epoch:376 ,loss:[0.0898870095872879], accuracy:[0.970900000038147] \n",
      "epoch:377 ,loss:[0.07860151905298234], accuracy:[0.973719999961853] \n",
      "epoch:378 ,loss:[0.042505297691822055], accuracy:[0.9867199999809265] \n",
      "epoch:379 ,loss:[0.05059701960682869], accuracy:[0.9835600000190735] \n",
      "epoch:380 ,loss:[0.03283741771772504], accuracy:[0.98952] \n",
      "epoch:381 ,loss:[0.019707755567878484], accuracy:[0.99442] \n",
      "epoch:382 ,loss:[0.019251518028080464], accuracy:[0.9942] \n",
      "epoch:383 ,loss:[0.02020203000843525], accuracy:[0.9934200000190735] \n",
      "epoch:384 ,loss:[0.02028745615661144], accuracy:[0.9938] \n",
      "epoch:385 ,loss:[0.023840520342588423], accuracy:[0.9926400000190735] \n",
      "epoch:386 ,loss:[0.04146488791465759], accuracy:[0.986139999961853] \n",
      "epoch:387 ,loss:[0.22672026124477387], accuracy:[0.9327199999809265] \n",
      "epoch:388 ,loss:[0.1525066082382202], accuracy:[0.95044] \n",
      "epoch:389 ,loss:[0.0772048270368576], accuracy:[0.9744800000381469] \n",
      "epoch:390 ,loss:[0.03994846291780472], accuracy:[0.9876800000381469] \n",
      "epoch:391 ,loss:[0.056711296353936194], accuracy:[0.9815000000190734] \n",
      "epoch:392 ,loss:[0.02490062527537346], accuracy:[0.9920000000190735] \n",
      "epoch:393 ,loss:[0.011774784328006207], accuracy:[0.99686] \n",
      "epoch:394 ,loss:[0.010865297484099866], accuracy:[0.99716] \n",
      "epoch:395 ,loss:[0.011093914057910442], accuracy:[0.99702] \n",
      "epoch:396 ,loss:[0.014265678149461745], accuracy:[0.99582] \n",
      "epoch:397 ,loss:[0.012514193775951862], accuracy:[0.99622] \n",
      "epoch:398 ,loss:[0.014293430191576481], accuracy:[0.99536] \n",
      "epoch:399 ,loss:[0.01799554554283619], accuracy:[0.9945600000190735] \n",
      "epoch:400 ,loss:[0.03274604876637459], accuracy:[0.989399999961853] \n",
      "epoch:401 ,loss:[0.05636052844047546], accuracy:[0.9818999999618531] \n",
      "epoch:402 ,loss:[0.11525981275081634], accuracy:[0.963560000038147] \n",
      "epoch:403 ,loss:[0.07987358164072036], accuracy:[0.97364] \n",
      "epoch:404 ,loss:[0.04441298680067062], accuracy:[0.9858000000381469] \n",
      "epoch:405 ,loss:[0.03972662592887878], accuracy:[0.987259999961853] \n",
      "epoch:406 ,loss:[0.06383356953620911], accuracy:[0.979260000038147] \n",
      "epoch:407 ,loss:[0.045466310496330264], accuracy:[0.98556] \n",
      "epoch:408 ,loss:[0.07063819341123104], accuracy:[0.9771] \n",
      "epoch:409 ,loss:[0.0861630127120018], accuracy:[0.971719999961853] \n",
      "epoch:410 ,loss:[0.044732149918079375], accuracy:[0.9853600000381469] \n",
      "epoch:411 ,loss:[0.026904374319165945], accuracy:[0.99122] \n",
      "epoch:412 ,loss:[0.014043256203942001], accuracy:[0.99602] \n",
      "epoch:413 ,loss:[0.011151394031643868], accuracy:[0.996720000038147] \n",
      "epoch:414 ,loss:[0.014218979788422584], accuracy:[0.99598] \n",
      "epoch:415 ,loss:[0.013698577677607536], accuracy:[0.9957400000190735] \n",
      "epoch:416 ,loss:[0.011185791738033295], accuracy:[0.9965000000190735] \n",
      "epoch:417 ,loss:[0.01357029149889946], accuracy:[0.9958000000190735] \n",
      "epoch:418 ,loss:[0.019096225652098654], accuracy:[0.99476] \n",
      "epoch:419 ,loss:[0.017643311830461025], accuracy:[0.99448] \n",
      "epoch:420 ,loss:[0.03747886488199234], accuracy:[0.9876799999618531] \n",
      "epoch:421 ,loss:[0.15437836040973663], accuracy:[0.9539200000190735] \n",
      "epoch:422 ,loss:[0.2192611516189575], accuracy:[0.93466] \n",
      "epoch:423 ,loss:[0.07938741574048996], accuracy:[0.974219999961853] \n",
      "epoch:424 ,loss:[0.049855930681228636], accuracy:[0.9841600000190734] \n",
      "epoch:425 ,loss:[0.03171830107092857], accuracy:[0.990500000038147] \n",
      "epoch:426 ,loss:[0.0404114352992177], accuracy:[0.98704] \n",
      "epoch:427 ,loss:[0.027340761020183565], accuracy:[0.9917000000190734] \n",
      "epoch:428 ,loss:[0.021462147332429887], accuracy:[0.993580000038147] \n",
      "epoch:429 ,loss:[0.05670499639123678], accuracy:[0.98134] \n",
      "epoch:430 ,loss:[0.023219005130529404], accuracy:[0.9931800000190735] \n",
      "epoch:431 ,loss:[0.021940892689526082], accuracy:[0.99352] \n",
      "epoch:432 ,loss:[0.01837931368112564], accuracy:[0.99448] \n",
      "epoch:433 ,loss:[0.014916083069294692], accuracy:[0.99578] \n",
      "epoch:434 ,loss:[0.008982071717195212], accuracy:[0.99766] \n",
      "epoch:435 ,loss:[0.00869326601203531], accuracy:[0.99788] \n",
      "epoch:436 ,loss:[0.009269054761528969], accuracy:[0.99732] \n",
      "epoch:437 ,loss:[0.011332884529232978], accuracy:[0.9966400000190735] \n",
      "epoch:438 ,loss:[0.022687997025176883], accuracy:[0.99232] \n",
      "epoch:439 ,loss:[0.07350222138881683], accuracy:[0.9769199999809265] \n",
      "epoch:440 ,loss:[0.156266405878067], accuracy:[0.9511400000190735] \n",
      "epoch:441 ,loss:[0.132781412858963], accuracy:[0.9587400000190734] \n",
      "epoch:442 ,loss:[0.07997005114078522], accuracy:[0.9734799999618531] \n",
      "epoch:443 ,loss:[0.043738954534828664], accuracy:[0.98616] \n",
      "epoch:444 ,loss:[0.01700603992640972], accuracy:[0.99512] \n",
      "epoch:445 ,loss:[0.014369301439523696], accuracy:[0.99582] \n",
      "epoch:446 ,loss:[0.009131406586207449], accuracy:[0.99752] \n",
      "epoch:447 ,loss:[0.010401795148253441], accuracy:[0.9971000000190735] \n",
      "epoch:448 ,loss:[0.014319283301532268], accuracy:[0.99608] \n",
      "epoch:449 ,loss:[0.0158390902620554], accuracy:[0.9951400000190734] \n",
      "epoch:450 ,loss:[0.007628588221883401], accuracy:[0.99786] \n",
      "epoch:451 ,loss:[0.006558801811411977], accuracy:[0.99826] \n",
      "epoch:452 ,loss:[0.0063570595434308055], accuracy:[0.99814] \n",
      "epoch:453 ,loss:[0.009946669647097587], accuracy:[0.9969200000190734] \n",
      "epoch:454 ,loss:[0.009287924257442355], accuracy:[0.99748] \n",
      "epoch:455 ,loss:[0.009213974675238133], accuracy:[0.99732] \n",
      "epoch:456 ,loss:[0.02365486998409033], accuracy:[0.99224] \n",
      "epoch:457 ,loss:[0.029038399783968924], accuracy:[0.990060000038147] \n",
      "epoch:458 ,loss:[0.11633073484301568], accuracy:[0.96518] \n",
      "epoch:459 ,loss:[0.11331374220848084], accuracy:[0.963880000038147] \n",
      "epoch:460 ,loss:[0.07261905987262726], accuracy:[0.976400000038147] \n",
      "epoch:461 ,loss:[0.07031074220836163], accuracy:[0.97644] \n",
      "epoch:462 ,loss:[0.04603844505786896], accuracy:[0.9855200000190735] \n",
      "epoch:463 ,loss:[0.04962616927340627], accuracy:[0.98406] \n",
      "epoch:464 ,loss:[0.03108256790280342], accuracy:[0.9903800000190734] \n",
      "epoch:465 ,loss:[0.027498403064198793], accuracy:[0.9917] \n",
      "epoch:466 ,loss:[0.014555411672145128], accuracy:[0.99586] \n",
      "epoch:467 ,loss:[0.01775677697479725], accuracy:[0.9947600000190735] \n",
      "epoch:468 ,loss:[0.008919206431955099], accuracy:[0.99742] \n",
      "epoch:469 ,loss:[0.0067757439579814675], accuracy:[0.99814] \n",
      "epoch:470 ,loss:[0.0077364653328061105], accuracy:[0.99804] \n",
      "epoch:471 ,loss:[0.016126791218817233], accuracy:[0.99506] \n",
      "epoch:472 ,loss:[0.016127771243974565], accuracy:[0.99508] \n",
      "epoch:473 ,loss:[0.01224039168998599], accuracy:[0.9965] \n",
      "epoch:474 ,loss:[0.012382802010327577], accuracy:[0.99632] \n",
      "epoch:475 ,loss:[0.02552422188103199], accuracy:[0.9920200000381469] \n",
      "epoch:476 ,loss:[0.1262525991344452], accuracy:[0.9630999999809265] \n",
      "epoch:477 ,loss:[0.22064636084079742], accuracy:[0.9353799999809265] \n",
      "epoch:478 ,loss:[0.11445809358000755], accuracy:[0.9639] \n",
      "epoch:479 ,loss:[0.021169850040078163], accuracy:[0.9944] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:480 ,loss:[0.01190139924377203], accuracy:[0.99716] \n",
      "epoch:481 ,loss:[0.012938855390548705], accuracy:[0.9966800000190735] \n",
      "epoch:482 ,loss:[0.01696247554242611], accuracy:[0.99562] \n",
      "epoch:483 ,loss:[0.010500663404464722], accuracy:[0.9972600000190734] \n",
      "epoch:484 ,loss:[0.01006838237222284], accuracy:[0.99726] \n",
      "epoch:485 ,loss:[0.009266416100114585], accuracy:[0.99762] \n",
      "epoch:486 ,loss:[0.010248966519832611], accuracy:[0.9973] \n",
      "epoch:487 ,loss:[0.00686337735325098], accuracy:[0.9982] \n",
      "epoch:488 ,loss:[0.00984545982658863], accuracy:[0.99726] \n",
      "epoch:489 ,loss:[0.006879549319613725], accuracy:[0.99838] \n",
      "epoch:490 ,loss:[0.004286252047270536], accuracy:[0.99902] \n",
      "epoch:491 ,loss:[0.005222838683351875], accuracy:[0.99862] \n",
      "epoch:492 ,loss:[0.01990989954918623], accuracy:[0.9936600000190735] \n",
      "epoch:493 ,loss:[0.06529475567102433], accuracy:[0.977940000038147] \n",
      "epoch:494 ,loss:[0.07658495907872916], accuracy:[0.97616] \n",
      "epoch:495 ,loss:[0.04520008447945118], accuracy:[0.9852] \n",
      "epoch:496 ,loss:[0.016073263800144195], accuracy:[0.995520000038147] \n",
      "epoch:497 ,loss:[0.03846001043528319], accuracy:[0.9873] \n",
      "epoch:498 ,loss:[0.017799934154450893], accuracy:[0.99444] \n",
      "epoch:499 ,loss:[0.01737631039261818], accuracy:[0.99484] \n"
     ]
    },
    {
     "ename": "MemoryError",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mMemoryError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-6-ad2fbb9ac002>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'CIFAR10_DNN2_LOSS'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mDNN2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloss\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'CIFAR10_DNN2_ACC'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mDNN2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maccuracy\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'CIFAR10_DNN1_WEIGHTS'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mDNN2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mweights\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'CIFAR10_DNN1_GRADS'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mDNN2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgrads\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/ML_env/lib/python3.6/site-packages/numpy/lib/npyio.py\u001b[0m in \u001b[0;36msave\u001b[0;34m(file, arr, allow_pickle, fix_imports)\u001b[0m\n\u001b[1;32m    525\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    526\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 527\u001b[0;31m         \u001b[0marr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0masanyarray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    528\u001b[0m         format.write_array(fid, arr, allow_pickle=allow_pickle,\n\u001b[1;32m    529\u001b[0m                            pickle_kwargs=pickle_kwargs)\n",
      "\u001b[0;32m~/anaconda3/envs/ML_env/lib/python3.6/site-packages/numpy/core/numeric.py\u001b[0m in \u001b[0;36masanyarray\u001b[0;34m(a, dtype, order)\u001b[0m\n\u001b[1;32m    589\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    590\u001b[0m     \"\"\"\n\u001b[0;32m--> 591\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcopy\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0morder\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0morder\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msubok\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    592\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    593\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mMemoryError\u001b[0m: "
     ]
    }
   ],
   "source": [
    "    DNN2 = MY_DNN2()\n",
    "    DNN2.train(x_train, y_train)\n",
    "    DNN2.model.save(\"CIFAR10_DNN2.h5\")\n",
    "    np.save('CIFAR10_DNN2_LOSS', DNN2.loss)\n",
    "    np.save('CIFAR10_DNN2_ACC', DNN2.accuracy)\n",
    "    np.save('CIFAR10_DNN2_WEIGHTS',DNN2.weights)\n",
    "    np.save('CIFAR10_DNN2_GRADS',DNN2.grads)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'MY_DNN2' object has no attribute 'loss'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-10-de8246001e3e>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mplot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mepoch_range\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mDNN2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloss\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mplot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mepoch_range\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mDNN2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maccuracy\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mplot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mepoch_range\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mDNN2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgrads\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'MY_DNN2' object has no attribute 'loss'"
     ]
    }
   ],
   "source": [
    "    plt.plot(range(epoch_range), DNN2.loss)\n",
    "    plt.plot(range(epoch_range), DNN2.accuracy)\n",
    "    plt.plot(range(epoch_range), DNN2.grads)\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
