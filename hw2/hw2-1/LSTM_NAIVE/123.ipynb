{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.layers import Input, Softmax, Dense, Dropout\n",
    "from tensorflow.keras.layers import CuDNNGRU, CuDNNLSTM\n",
    "from tensorflow.keras.layers import Bidirectional, TimeDistributed\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "import numpy as np\n",
    "import json\n",
    "import re\n",
    "\n",
    "import nltk\n",
    "from nltk import word_tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "idfilename = 'training_data/id.txt'\n",
    "datadirname = 'training_data/feat/'\n",
    "labelfilename = 'training_label.json'\n",
    "\n",
    "with open(\"DIC_word_index.json\") as f:\n",
    "    DIC_word_index = json.load(f)\n",
    "    \n",
    "with open(\"DIC_index_word.json\") as f:\n",
    "    DIC_index_word = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# loading training data\n",
    "encode_x = []\n",
    "video_id = {}\n",
    "for i,video_name in enumerate(open(idfilename)):\n",
    "    video_name = video_name[:-1]\n",
    "    x = np.load(datadirname + video_name + \".npy\")\n",
    "    encode_x.append(x)\n",
    "    video_id[video_name] = i\n",
    "\n",
    "encode_x = np.array(encode_x)\n",
    "    \n",
    "TRAIN_SZ = len(encode_x)\n",
    "decode_x = [[]]*TRAIN_SZ\n",
    "decode_y = [[]]*TRAIN_SZ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "def Sent2Seq(sent):\n",
    "    #print(sent)    \n",
    "    tokens = word_tokenize(sent.lower())\n",
    "    ret = []\n",
    "    for word in tokens:\n",
    "        ret.append(DIC_word_index[word])\n",
    "    return ret\n",
    "\n",
    "BOS = \"<bos>\" # index is 1\n",
    "EOS = \"<eos>\" # index is 2\n",
    "\n",
    "VOCAB_SZ = len(DIC_word_index)\n",
    "MAX_SEQ_LEN = 0;\n",
    "# loading decoder data\n",
    "rawlabels = json.load(open(labelfilename, 'r'))\n",
    "for data in rawlabels:    \n",
    "    index = video_id[data['id']]\n",
    "    #print(index)\n",
    "    sent =  data['caption'][0] # select one sentence for now\n",
    "    # TODO: implement Sent2Seq\n",
    "    decode_x[index] = [1] + Sent2Seq(sent)\n",
    "    decode_y[index] = Sent2Seq(sent) + [2]\n",
    "    if(len(decode_x[index]) > MAX_SEQ_LEN):\n",
    "        MAX_SEQ_LEN = len(decode_x[index])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1, 4, 5, 6, 7, 4, 8, 9]\n",
      "[4, 5, 6, 7, 4, 8, 9, 2]\n"
     ]
    }
   ],
   "source": [
    "print(decode_x[0])\n",
    "print(decode_y[0]) #one shift from decode_x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "\n",
    "decode_x = pad_sequences(decode_x, maxlen=MAX_SEQ_LEN, padding='post', truncating='pre')\n",
    "decode_y = pad_sequences(decode_y, maxlen=MAX_SEQ_LEN, padding='post', truncating='pre')\n",
    "decode_y = to_categorical(decode_y, num_classes=VOCAB_SZ)\n",
    "decode_x = to_categorical(decode_x, num_classes=VOCAB_SZ)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1450, 80, 4096)\n",
      "(1450, 45, 6087)\n",
      "(1450, 45, 6087)\n"
     ]
    }
   ],
   "source": [
    "print(encode_x.shape)\n",
    "print(decode_x.shape)\n",
    "print(decode_y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "latent_dim = 256\n",
    "#=================encoder====================#\n",
    "encoder_inputs = Input(shape=(80, 4096))\n",
    "encoder = CuDNNLSTM(latent_dim, return_sequences=True, return_state=True)\n",
    "encoder_outputs, state_h, state_c = encoder(encoder_inputs)\n",
    "encoder_states = [state_h, state_c]\n",
    "#=================decoder====================#\n",
    "decoder_inputs = Input(shape=(None,VOCAB_SZ))\n",
    "decoder_lstm = CuDNNLSTM(latent_dim, return_sequences=True, return_state=True)\n",
    "decoder_outputs,_ , _ = decoder_lstm(decoder_inputs, initial_state=encoder_states)\n",
    "\n",
    "decoder_dense = Dense(VOCAB_SZ, activation='softmax')\n",
    "decoder_outputs = decoder_dense(decoder_outputs)\n",
    "#=============================================\n",
    "model = Model([encoder_inputs, decoder_inputs], decoder_outputs)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_3 (InputLayer)            (None, 80, 4096)     0                                            \n",
      "__________________________________________________________________________________________________\n",
      "input_4 (InputLayer)            (None, None, 6087)   0                                            \n",
      "__________________________________________________________________________________________________\n",
      "cu_dnnlstm_3 (CuDNNLSTM)        [(None, 80, 256), (N 4458496     input_3[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "cu_dnnlstm_4 (CuDNNLSTM)        [(None, None, 256),  6497280     input_4[0][0]                    \n",
      "                                                                 cu_dnnlstm_3[0][1]               \n",
      "                                                                 cu_dnnlstm_3[0][2]               \n",
      "__________________________________________________________________________________________________\n",
      "dense (Dense)                   (None, None, 6087)   1564359     cu_dnnlstm_4[0][0]               \n",
      "==================================================================================================\n",
      "Total params: 12,520,135\n",
      "Trainable params: 12,520,135\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 1305 samples, validate on 145 samples\n",
      "Epoch 1/50\n",
      "1305/1305 [==============================] - 45s 34ms/step - loss: 1.4468 - acc: 0.7960 - val_loss: 1.2231 - val_acc: 0.8230\n",
      "Epoch 2/50\n",
      "1305/1305 [==============================] - 41s 31ms/step - loss: 1.0792 - acc: 0.8286 - val_loss: 1.1591 - val_acc: 0.8282\n",
      "Epoch 3/50\n",
      "1153/1305 [=========================>....] - ETA: 4s - loss: 0.9701 - acc: 0.8360"
     ]
    }
   ],
   "source": [
    "optimizer = Adam(lr=0.001)\n",
    "model.compile(optimizer=optimizer, loss='categorical_crossentropy', metrics=[\"accuracy\"])\n",
    "model.fit([encode_x, decode_x], decode_y, validation_split=0.1, batch_size=1, epochs=50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#==============inference setup===================#\n",
    "encoder_model = Model(encoder_inputs, encoder_states)\n",
    "\n",
    "decoder_state_input_h = Input(shape=(latent_dim,))\n",
    "decoder_state_input_c = Input(shape=(latent_dim,))\n",
    "decoder_states_inputs = [decoder_state_input_h, decoder_state_input_c]\n",
    "decoder_outputs, state_h, state_c = decoder_lstm(\n",
    "    decoder_inputs, initial_state=decoder_states_inputs)\n",
    "decoder_states = [state_h, state_c]\n",
    "decoder_outputs = decoder_dense(decoder_outputs)\n",
    "decoder_model = Model(\n",
    "    [decoder_inputs] + decoder_states_inputs,\n",
    "    [decoder_outputs] + decoder_states)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def decode_sequence(input_seq):\n",
    "    # Encode the input as state vectors.\n",
    "    states_value = encoder_model.predict(input_seq)\n",
    "    # Generate empty decode_y seq\n",
    "    # y_seq shape : (1, 1, 6087)\n",
    "    y_seq = np.zeros((1, 1, VOCAB_SZ))\n",
    "    y_seq[0, 0, DIC_word_index[\"<bos>\"]] = 1\n",
    "    \n",
    "    stop_condition = False\n",
    "    decoded_sentence = []\n",
    "    while not stop_condition:\n",
    "        #output_tokens shape : (1, 1, 6087)\n",
    "        #output_tokens[0, -1, :] shape : (6087, )\n",
    "        output_tokens, h, c = decoder_model.predict(  \n",
    "            [y_seq] + states_value)\n",
    "        sampled_token_index = np.argmax(output_tokens[0, -1, :] )\n",
    "        sampled_token = DIC_index_word[str(sampled_token_index)]\n",
    "        if(sampled_token!='<eos>'):\n",
    "            decoded_sentence.append(sampled_token)\n",
    "        \n",
    "        #Exit Condition :either hit max length or find stop char\n",
    "        \n",
    "        if(sampled_token == '<eos>' or\n",
    "          len(decoded_sentence) > MAX_SEQ_LEN):\n",
    "            stop_condition = True\n",
    "        \n",
    "        #Update y_seq\n",
    "        y_seq = np.zeros((1, 1, VOCAB_SZ))\n",
    "        y_seq[0, 0, sampled_token_index] = 1\n",
    "        \n",
    "        #Update states\n",
    "        states_value = [h, c]\n",
    "        \n",
    "    return decoded_sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "idfilename = 'testing_data/id.txt'\n",
    "datadirname = 'testing_data/feat/'\n",
    "labelfilename = 'testing_label.json'\n",
    "\n",
    "encode_x = []\n",
    "video_id = []\n",
    "for i,lb in enumerate(open(idfilename)):\n",
    "    lb = lb[:-1]\n",
    "    encode_x.append(np.load(datadirname + lb +\".npy\"))\n",
    "    video_id.append(lb)\n",
    "    \n",
    "out_labels = []\n",
    "for indexx in range(len(encode_x)):\n",
    "    sent = decode_sequence(np.array([encode_x[indexx]]))\n",
    "    sent = \" \".join(sent)\n",
    "    print(sent)\n",
    "    out_labels.append(sent)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('MODELTEST_testing.txt', 'w') as f:\n",
    "    for i in range(len(encode_x)):\n",
    "        f.write(video_id[i] + ',' + out_labels[i] + '\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#====================TESTING===================#\n",
    "\n",
    "idfilename_t = 'testing_data/id.txt'\n",
    "datadirname_t = 'testing_data/feat/'\n",
    "\n",
    "# loading testing data\n",
    "encode_x_t = []\n",
    "video_id_t = {}\n",
    "for i,video_name in enumerate(open(idfilename_t)):\n",
    "    #lb contains '\\n', therefore lb[:-1]\n",
    "    video_name = video_name[:-1]\n",
    "    x = np.load(datadirname_t + video_name + \".npy\")\n",
    "    encode_x_t.append(x)\n",
    "    video_id_t[video_name] = i"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#============predict the caption============#\n",
    "OUTPUTS = []\n",
    "for X in encode_x_t:\n",
    "    X = np.array([X])\n",
    "    Y = decode_sequence(X)\n",
    "    OUTPUTS.append(Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#============to json============#\n",
    "predict_label = []\n",
    "with open('predict_label.txt', 'w') as f:\n",
    "    for video_name, _id in video_id_t.items():\n",
    "        tokens = OUTPUTS[int(_id)][:-2]\n",
    "        predict = \" \".join(tokens)\n",
    "        predict +=\".\"\n",
    "\n",
    "        f.write(str(video_name) + \",\" + predict +\"\\n\")\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "video_id_t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
