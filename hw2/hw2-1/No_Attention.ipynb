{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.layers import Input, Softmax, Dense, Dropout\n",
    "from tensorflow.keras.layers import CuDNNGRU, CuDNNLSTM\n",
    "from tensorflow.keras.layers import Bidirectional, TimeDistributed\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "import numpy as np\n",
    "import json\n",
    "import re\n",
    "\n",
    "from tensorflow.keras.callbacks import EarlyStopping,ReduceLROnPlateau\n",
    "\n",
    "from keras import regularizers\n",
    "\n",
    "import nltk\n",
    "from nltk import word_tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "idfilename = 'training_data/id.txt'\n",
    "datadirname = 'training_data/feat/'\n",
    "labelfilename = 'training_label.json'\n",
    "\n",
    "with open(\"DIC_word_index.json\") as f:\n",
    "    DIC_word_index = json.load(f)\n",
    "    \n",
    "with open(\"DIC_index_word.json\") as f:\n",
    "    DIC_index_word = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# loading training data\n",
    "encode_x = []\n",
    "video_id = {}\n",
    "for i,video_name in enumerate(open(idfilename)):\n",
    "    video_name = video_name[:-1]\n",
    "    x = np.load(datadirname + video_name + \".npy\")\n",
    "    encode_x.append(x)\n",
    "    video_id[video_name] = i\n",
    "\n",
    "encode_x = np.array(encode_x)\n",
    "    \n",
    "TRAIN_SZ = len(encode_x)\n",
    "decode_x = [[]]*TRAIN_SZ\n",
    "decode_y = [[]]*TRAIN_SZ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "def Sent2Seq(sent):\n",
    "    #print(sent)    \n",
    "    tokens = word_tokenize(sent.lower())\n",
    "    ret = []\n",
    "    for word in tokens:\n",
    "        ret.append(DIC_word_index[word])\n",
    "    return ret\n",
    "\n",
    "BOS = \"<bos>\" # index is 1\n",
    "EOS = \"<eos>\" # index is 2\n",
    "\n",
    "VOCAB_SZ = len(DIC_word_index)\n",
    "MAX_SEQ_LEN = 0;\n",
    "# loading decoder data\n",
    "rawlabels = json.load(open(labelfilename, 'r'))\n",
    "for data in rawlabels:    \n",
    "    index = video_id[data['id']]\n",
    "    #print(index)\n",
    "    sent =  data['caption'][0] # select one sentence for now\n",
    "    # TODO: implement Sent2Seq\n",
    "    decode_x[index] = [1] + Sent2Seq(sent)\n",
    "    decode_y[index] = Sent2Seq(sent) + [2]\n",
    "    if(len(decode_x[index]) > MAX_SEQ_LEN):\n",
    "        MAX_SEQ_LEN = len(decode_x[index])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1, 4, 5, 6, 7, 4, 8, 9]\n",
      "[4, 5, 6, 7, 4, 8, 9, 2]\n"
     ]
    }
   ],
   "source": [
    "print(decode_x[0])\n",
    "print(decode_y[0]) #one shift from decode_x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "\n",
    "MAX_SEQ_LEN = 15\n",
    "\n",
    "decode_x = pad_sequences(decode_x, maxlen=MAX_SEQ_LEN, padding='post', truncating='pre')\n",
    "decode_y = pad_sequences(decode_y, maxlen=MAX_SEQ_LEN, padding='post', truncating='pre')\n",
    "decode_y = to_categorical(decode_y, num_classes=VOCAB_SZ)\n",
    "decode_x = to_categorical(decode_x, num_classes=VOCAB_SZ)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1450, 80, 4096)\n",
      "(1450, 15, 6087)\n",
      "(1450, 15, 6087)\n"
     ]
    }
   ],
   "source": [
    "print(encode_x.shape)\n",
    "print(decode_x.shape)\n",
    "print(decode_y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "latent_dim = 256\n",
    "#=================encoder====================#\n",
    "encoder_inputs = Input(shape=(80, 4096))\n",
    "encoder = CuDNNLSTM(latent_dim, return_sequences=True, return_state=True)\n",
    "encoder_outputs, state_h, state_c = encoder(encoder_inputs)\n",
    "encoder_states = [state_h, state_c]\n",
    "#=================decoder====================#\n",
    "decoder_inputs = Input(shape=(None,VOCAB_SZ))\n",
    "decoder_lstm = CuDNNLSTM(latent_dim, return_sequences=True, return_state=True)\n",
    "decoder_outputs,_ , _ = decoder_lstm(decoder_inputs, initial_state=encoder_states)\n",
    "\n",
    "decoder_dense = Dense(VOCAB_SZ, activation='softmax', kernel_regularizer=regularizers.l2(0.01),\n",
    "                activity_regularizer=regularizers.l1(0.01))\n",
    "decoder_outputs = decoder_dense(decoder_outputs)\n",
    "#=============================================\n",
    "model = Model([encoder_inputs, decoder_inputs], decoder_outputs)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_1 (InputLayer)            (None, 80, 4096)     0                                            \n",
      "__________________________________________________________________________________________________\n",
      "input_2 (InputLayer)            (None, None, 6087)   0                                            \n",
      "__________________________________________________________________________________________________\n",
      "cu_dnnlstm (CuDNNLSTM)          [(None, 80, 256), (N 4458496     input_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "cu_dnnlstm_1 (CuDNNLSTM)        [(None, None, 256),  6497280     input_2[0][0]                    \n",
      "                                                                 cu_dnnlstm[0][1]                 \n",
      "                                                                 cu_dnnlstm[0][2]                 \n",
      "__________________________________________________________________________________________________\n",
      "dense (Dense)                   (None, None, 6087)   1564359     cu_dnnlstm_1[0][0]               \n",
      "==================================================================================================\n",
      "Total params: 12,520,135\n",
      "Trainable params: 12,520,135\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 1305 samples, validate on 145 samples\n",
      "Epoch 1/50\n",
      "1305/1305 [==============================] - 17s 13ms/step - loss: 4.3409 - acc: 0.4282 - val_loss: 4.0182 - val_acc: 0.5025\n",
      "Epoch 2/50\n",
      "1305/1305 [==============================] - 15s 11ms/step - loss: 3.7068 - acc: 0.5252 - val_loss: 3.8304 - val_acc: 0.5172\n",
      "Epoch 3/50\n",
      "1305/1305 [==============================] - 15s 12ms/step - loss: 3.5495 - acc: 0.5386 - val_loss: 3.7451 - val_acc: 0.5223\n",
      "Epoch 4/50\n",
      "1305/1305 [==============================] - 15s 11ms/step - loss: 3.4535 - acc: 0.5449 - val_loss: 3.7597 - val_acc: 0.5191\n",
      "Epoch 5/50\n",
      "1305/1305 [==============================] - 15s 11ms/step - loss: 3.3851 - acc: 0.5505 - val_loss: 3.7321 - val_acc: 0.5315\n",
      "Epoch 6/50\n",
      "1305/1305 [==============================] - 15s 12ms/step - loss: 3.3410 - acc: 0.5545 - val_loss: 3.7148 - val_acc: 0.5324\n",
      "Epoch 7/50\n",
      "1305/1305 [==============================] - 15s 12ms/step - loss: 3.3035 - acc: 0.5562 - val_loss: 3.7488 - val_acc: 0.5241\n",
      "Epoch 8/50\n",
      "1305/1305 [==============================] - 15s 11ms/step - loss: 3.2712 - acc: 0.5601 - val_loss: 3.7423 - val_acc: 0.5333\n",
      "Epoch 9/50\n",
      "1305/1305 [==============================] - 15s 11ms/step - loss: 3.2388 - acc: 0.5613 - val_loss: 3.7437 - val_acc: 0.5366\n",
      "Epoch 10/50\n",
      "1305/1305 [==============================] - 14s 11ms/step - loss: 3.2098 - acc: 0.5638 - val_loss: 3.7675 - val_acc: 0.5310\n",
      "Epoch 11/50\n",
      "1305/1305 [==============================] - 15s 11ms/step - loss: 3.1826 - acc: 0.5666 - val_loss: 3.7674 - val_acc: 0.5366\n",
      "Epoch 12/50\n",
      "1305/1305 [==============================] - 14s 11ms/step - loss: 3.1576 - acc: 0.5699 - val_loss: 3.8251 - val_acc: 0.5430\n",
      "Epoch 13/50\n",
      "1305/1305 [==============================] - 15s 11ms/step - loss: 3.1379 - acc: 0.5710 - val_loss: 3.8074 - val_acc: 0.5398\n",
      "Epoch 14/50\n",
      "1305/1305 [==============================] - 15s 12ms/step - loss: 3.1158 - acc: 0.5739 - val_loss: 3.8028 - val_acc: 0.5379\n",
      "Epoch 15/50\n",
      "1305/1305 [==============================] - 15s 12ms/step - loss: 3.0942 - acc: 0.5750 - val_loss: 3.8062 - val_acc: 0.5329\n",
      "Epoch 16/50\n",
      "1305/1305 [==============================] - 15s 12ms/step - loss: 3.0770 - acc: 0.5785 - val_loss: 3.8084 - val_acc: 0.5301\n",
      "Epoch 17/50\n",
      "1305/1305 [==============================] - 15s 11ms/step - loss: 3.0609 - acc: 0.5808 - val_loss: 3.8134 - val_acc: 0.5398\n",
      "Epoch 18/50\n",
      "1305/1305 [==============================] - 15s 11ms/step - loss: 3.0422 - acc: 0.5840 - val_loss: 3.8251 - val_acc: 0.5398\n",
      "Epoch 19/50\n",
      "1305/1305 [==============================] - 15s 11ms/step - loss: 3.0273 - acc: 0.5844 - val_loss: 3.8275 - val_acc: 0.5370\n",
      "Epoch 20/50\n",
      "1305/1305 [==============================] - 14s 11ms/step - loss: 3.0121 - acc: 0.5876 - val_loss: 3.8047 - val_acc: 0.5384\n",
      "Epoch 21/50\n",
      "1305/1305 [==============================] - 14s 11ms/step - loss: 3.0000 - acc: 0.5894 - val_loss: 3.8075 - val_acc: 0.5398\n",
      "Epoch 22/50\n",
      "1305/1305 [==============================] - 14s 11ms/step - loss: 2.9840 - acc: 0.5920 - val_loss: 3.8392 - val_acc: 0.5292\n",
      "Epoch 23/50\n",
      "1305/1305 [==============================] - 14s 11ms/step - loss: 2.9746 - acc: 0.5947 - val_loss: 3.8171 - val_acc: 0.5384\n",
      "Epoch 24/50\n",
      "1305/1305 [==============================] - 14s 11ms/step - loss: 2.9605 - acc: 0.5972 - val_loss: 3.8189 - val_acc: 0.5324\n",
      "Epoch 25/50\n",
      "1305/1305 [==============================] - 14s 11ms/step - loss: 2.9490 - acc: 0.5992 - val_loss: 3.8195 - val_acc: 0.5352\n",
      "Epoch 26/50\n",
      "1305/1305 [==============================] - 14s 11ms/step - loss: 2.9364 - acc: 0.5994 - val_loss: 3.8137 - val_acc: 0.5402\n",
      "Epoch 27/50\n",
      "1305/1305 [==============================] - 14s 11ms/step - loss: 2.9251 - acc: 0.6046 - val_loss: 3.8403 - val_acc: 0.5389\n",
      "Epoch 28/50\n",
      "1305/1305 [==============================] - 15s 11ms/step - loss: 2.9158 - acc: 0.6041 - val_loss: 3.8389 - val_acc: 0.5310\n",
      "Epoch 29/50\n",
      "1305/1305 [==============================] - 14s 11ms/step - loss: 2.9070 - acc: 0.6071 - val_loss: 3.8463 - val_acc: 0.5329\n",
      "Epoch 30/50\n",
      "1305/1305 [==============================] - 14s 11ms/step - loss: 2.8973 - acc: 0.6088 - val_loss: 3.8264 - val_acc: 0.5338\n",
      "Epoch 31/50\n",
      "1305/1305 [==============================] - 14s 11ms/step - loss: 2.8852 - acc: 0.6106 - val_loss: 3.8243 - val_acc: 0.5407\n",
      "Epoch 32/50\n",
      "1305/1305 [==============================] - 14s 11ms/step - loss: 2.8765 - acc: 0.6125 - val_loss: 3.8147 - val_acc: 0.5333\n",
      "Epoch 33/50\n",
      "1305/1305 [==============================] - 14s 11ms/step - loss: 2.8691 - acc: 0.6129 - val_loss: 3.8581 - val_acc: 0.5306\n",
      "Epoch 34/50\n",
      "1305/1305 [==============================] - 15s 11ms/step - loss: 2.8617 - acc: 0.6159 - val_loss: 3.8487 - val_acc: 0.5320\n",
      "Epoch 35/50\n",
      "1305/1305 [==============================] - 15s 12ms/step - loss: 2.8529 - acc: 0.6172 - val_loss: 3.8652 - val_acc: 0.5370\n",
      "Epoch 36/50\n",
      "1305/1305 [==============================] - 15s 12ms/step - loss: 2.8448 - acc: 0.6191 - val_loss: 3.8520 - val_acc: 0.5297\n",
      "Epoch 37/50\n",
      "1305/1305 [==============================] - 15s 12ms/step - loss: 2.8365 - acc: 0.6201 - val_loss: 3.8540 - val_acc: 0.5366\n",
      "Epoch 38/50\n",
      "1305/1305 [==============================] - 15s 11ms/step - loss: 2.8341 - acc: 0.6220 - val_loss: 3.8533 - val_acc: 0.5315\n",
      "Epoch 39/50\n",
      "1305/1305 [==============================] - 14s 11ms/step - loss: 2.8247 - acc: 0.6227 - val_loss: 3.8772 - val_acc: 0.5320\n",
      "Epoch 40/50\n",
      "1305/1305 [==============================] - 14s 11ms/step - loss: 2.8188 - acc: 0.6249 - val_loss: 3.8501 - val_acc: 0.5274\n",
      "Epoch 41/50\n",
      "1305/1305 [==============================] - 14s 11ms/step - loss: 2.8120 - acc: 0.6255 - val_loss: 3.8554 - val_acc: 0.5306\n",
      "Epoch 42/50\n",
      "1305/1305 [==============================] - 14s 11ms/step - loss: 2.8075 - acc: 0.6276 - val_loss: 3.8607 - val_acc: 0.5333\n",
      "Epoch 43/50\n",
      "1305/1305 [==============================] - 15s 11ms/step - loss: 2.7990 - acc: 0.6290 - val_loss: 3.8789 - val_acc: 0.5246\n",
      "Epoch 44/50\n",
      "1305/1305 [==============================] - 15s 11ms/step - loss: 2.7897 - acc: 0.6318 - val_loss: 3.9048 - val_acc: 0.5223\n",
      "Epoch 45/50\n",
      "1305/1305 [==============================] - 14s 11ms/step - loss: 2.7852 - acc: 0.6328 - val_loss: 3.8772 - val_acc: 0.5297\n",
      "Epoch 46/50\n",
      "1305/1305 [==============================] - 15s 11ms/step - loss: 2.7752 - acc: 0.6348 - val_loss: 3.9194 - val_acc: 0.5228\n",
      "Epoch 47/50\n",
      "1305/1305 [==============================] - 15s 12ms/step - loss: 2.7690 - acc: 0.6368 - val_loss: 3.8747 - val_acc: 0.5255\n",
      "Epoch 48/50\n",
      "1305/1305 [==============================] - 15s 12ms/step - loss: 2.7722 - acc: 0.6364 - val_loss: 3.9121 - val_acc: 0.5195\n",
      "Epoch 49/50\n",
      "1305/1305 [==============================] - 15s 12ms/step - loss: 2.7567 - acc: 0.6376 - val_loss: 3.8718 - val_acc: 0.5329\n",
      "Epoch 50/50\n",
      "1305/1305 [==============================] - 15s 11ms/step - loss: 2.7601 - acc: 0.6394 - val_loss: 3.9479 - val_acc: 0.5269\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x7f6eb6a319b0>"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "optimizer = Adam(lr=0.001)\n",
    "\n",
    "lrreduc = ReduceLROnPlateau(monitor='loss', factor=0.5,\\\n",
    "                             patience=5, min_lr=0.00001, verbose=1, cooldown=5)\n",
    "model.compile(optimizer=optimizer, loss='categorical_crossentropy', metrics=[\"accuracy\"])\n",
    "\n",
    "model.fit([encode_x, decode_x], decode_y, validation_split=0.1, batch_size=1, epochs=50, callbacks=[lrreduc])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#==============inference setup===================#\n",
    "encoder_model = Model(encoder_inputs, encoder_states)\n",
    "\n",
    "decoder_state_input_h = Input(shape=(latent_dim,))\n",
    "decoder_state_input_c = Input(shape=(latent_dim,))\n",
    "decoder_states_inputs = [decoder_state_input_h, decoder_state_input_c]\n",
    "decoder_outputs, state_h, state_c = decoder_lstm(\n",
    "    decoder_inputs, initial_state=decoder_states_inputs)\n",
    "decoder_states = [state_h, state_c]\n",
    "decoder_outputs = decoder_dense(decoder_outputs)\n",
    "decoder_model = Model(\n",
    "    [decoder_inputs] + decoder_states_inputs,\n",
    "    [decoder_outputs] + decoder_states)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def decode_sequence(input_seq):\n",
    "    # Encode the input as state vectors.\n",
    "    states_value = encoder_model.predict(input_seq)\n",
    "    # Generate empty decode_y seq\n",
    "    # y_seq shape : (1, 1, 6087)\n",
    "    y_seq = np.zeros((1, 1, VOCAB_SZ))\n",
    "    y_seq[0, 0, DIC_word_index[\"<bos>\"]] = 1\n",
    "    \n",
    "    stop_condition = False\n",
    "    decoded_sentence = []\n",
    "    while not stop_condition:\n",
    "        #output_tokens shape : (1, 1, 6087)\n",
    "        #output_tokens[0, -1, :] shape : (6087, )\n",
    "        output_tokens, h, c = decoder_model.predict(  \n",
    "            [y_seq] + states_value)\n",
    "        sampled_token_index = np.argmax(output_tokens[0, -1, :] )\n",
    "        sampled_token = DIC_index_word[str(sampled_token_index)]\n",
    "        if(sampled_token!='<eos>'):\n",
    "            decoded_sentence.append(sampled_token)\n",
    "        \n",
    "        #Exit Condition :either hit max length or find stop char\n",
    "        \n",
    "        if(sampled_token == '<eos>' or\n",
    "          len(decoded_sentence) > MAX_SEQ_LEN):\n",
    "            stop_condition = True\n",
    "        \n",
    "        #Update y_seq\n",
    "        y_seq = np.zeros((1, 1, VOCAB_SZ))\n",
    "        y_seq[0, 0, sampled_token_index] = 1\n",
    "        \n",
    "        #Update states\n",
    "        states_value = [h, c]\n",
    "        \n",
    "    return decoded_sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "a woman is playing a .\n",
      "a man is playing a .\n",
      "a person is playing a .\n",
      "a woman is playing a .\n",
      "a woman is playing a .\n",
      "a person is playing .\n",
      "a person is playing .\n",
      "a cat is playing .\n",
      "a man is playing a .\n",
      "a woman is cutting a .\n",
      "someone is cutting a bowl of a bowl .\n",
      "a woman is playing a .\n",
      "a man is playing a .\n",
      "a woman is playing a .\n",
      "someone is cutting a bowl of a bowl .\n",
      "a man is playing a .\n",
      "a man is playing a .\n",
      "a man is playing a .\n",
      "a man is playing a a .\n",
      "a woman is playing a .\n",
      "a man is playing a .\n",
      "a man is playing .\n",
      "a man is playing a large .\n",
      "a person is playing a .\n",
      "a woman is cutting a bowl .\n",
      "a person is playing a .\n",
      "a woman is playing a .\n",
      "a person is playing a .\n",
      "a man is playing a .\n",
      "a man is playing a .\n",
      "a woman is cutting a bowl .\n",
      "a woman is cutting a .\n",
      "a man is playing a .\n",
      "a man is playing a .\n",
      "a woman is cutting a bowl .\n",
      "a man is playing a .\n",
      "a man is playing a .\n",
      "a man is playing a large .\n",
      "a boy is playing .\n",
      "a woman is cutting a bowl .\n",
      "two man is playing a .\n",
      "a woman is cutting a .\n",
      "a man is playing a .\n",
      "a person is playing a .\n",
      "someone is playing a into of .\n",
      "a woman is playing a .\n",
      "a person is playing .\n",
      "a woman is playing a .\n",
      "a woman is cutting a bowl .\n",
      "a man is playing a .\n",
      "a man is playing a .\n",
      "a man is playing a .\n",
      "a man is playing a .\n",
      "a woman is cutting a .\n",
      "a woman is playing a large .\n",
      "a woman is cutting a .\n",
      "a man is playing a .\n",
      "a man is playing a .\n",
      "a woman is cutting a bowl .\n",
      "a woman is cutting a .\n",
      "a man is playing a .\n",
      "a woman is cutting a .\n",
      "a woman is playing a .\n",
      "a woman is playing .\n",
      "a man is playing a .\n",
      "a woman is playing a .\n",
      "someone is cutting a bowl of a bowl .\n",
      "a person is playing .\n",
      "a woman is cutting a .\n",
      "a man is playing a .\n",
      "a man is playing a .\n",
      "a man is playing a .\n",
      "a woman is playing a .\n",
      "a woman is playing a .\n",
      "a woman is playing a .\n",
      "a woman is cutting a bowl .\n",
      "a woman is playing a .\n",
      "a woman is playing a .\n",
      "a woman is cutting a bowl .\n",
      "a person playing a man .\n",
      "a man is playing a .\n",
      "a man is playing .\n",
      "a woman is playing a .\n",
      "a woman is cutting a .\n",
      "a man is playing a .\n",
      "a person is playing a .\n",
      "a woman is playing a .\n",
      "a person is playing .\n",
      "a woman is playing with a .\n",
      "someone is cutting a bowl of a bowl .\n",
      "a person is playing a .\n",
      "a man is playing a .\n",
      "a man is playing a .\n",
      "a woman is cutting a .\n",
      "a person is playing a .\n",
      "a man is playing a .\n",
      "two are playing .\n",
      "a person is playing a .\n",
      "a woman is playing a .\n",
      "a woman is cutting a .\n"
     ]
    }
   ],
   "source": [
    "idfilename = 'testing_data/id.txt'\n",
    "datadirname = 'testing_data/feat/'\n",
    "labelfilename = 'testing_label.json'\n",
    "\n",
    "encode_x = []\n",
    "video_id = []\n",
    "for i,lb in enumerate(open(idfilename)):\n",
    "    lb = lb[:-1]\n",
    "    encode_x.append(np.load(datadirname + lb +\".npy\"))\n",
    "    video_id.append(lb)\n",
    "    \n",
    "out_labels = []\n",
    "for indexx in range(len(encode_x)):\n",
    "    sent = decode_sequence(np.array([encode_x[indexx]]))\n",
    "    sent = \" \".join(sent)\n",
    "    print(sent)\n",
    "    out_labels.append(sent)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('MODELTEST_testing.txt', 'w') as f:\n",
    "    for i in range(len(encode_x)):\n",
    "        f.write(video_id[i] + ',' + out_labels[i] + '\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "#====================TESTING===================#\n",
    "\n",
    "idfilename_t = 'testing_data/id.txt'\n",
    "datadirname_t = 'testing_data/feat/'\n",
    "\n",
    "# loading testing data\n",
    "encode_x_t = []\n",
    "video_id_t = {}\n",
    "for i,video_name in enumerate(open(idfilename_t)):\n",
    "    #lb contains '\\n', therefore lb[:-1]\n",
    "    video_name = video_name[:-1]\n",
    "    x = np.load(datadirname_t + video_name + \".npy\")\n",
    "    encode_x_t.append(x)\n",
    "    video_id_t[video_name] = i"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "#============predict the caption============#\n",
    "OUTPUTS = []\n",
    "for X in encode_x_t:\n",
    "    X = np.array([X])\n",
    "    Y = decode_sequence(X)\n",
    "    OUTPUTS.append(Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['a', 'woman', 'is', 'playing', 'a', '.']\n",
      "a woman is playing a \n",
      "['a', 'man', 'is', 'playing', 'a', '.']\n",
      "a man is playing a \n",
      "['a', 'person', 'is', 'playing', 'a', '.']\n",
      "a person is playing a \n",
      "['a', 'woman', 'is', 'playing', 'a', '.']\n",
      "a woman is playing a \n",
      "['a', 'woman', 'is', 'playing', 'a', '.']\n",
      "a woman is playing a \n",
      "['a', 'person', 'is', 'playing', '.']\n",
      "a person is playing \n",
      "['a', 'person', 'is', 'playing', '.']\n",
      "a person is playing \n",
      "['a', 'cat', 'is', 'playing', '.']\n",
      "a cat is playing \n",
      "['a', 'man', 'is', 'playing', 'a', '.']\n",
      "a man is playing a \n",
      "['a', 'woman', 'is', 'cutting', 'a', '.']\n",
      "a woman is cutting a \n",
      "['someone', 'is', 'cutting', 'a', 'bowl', 'of', 'a', 'bowl', '.']\n",
      "someone is cutting a bowl of a bowl \n",
      "['a', 'woman', 'is', 'playing', 'a', '.']\n",
      "a woman is playing a \n",
      "['a', 'man', 'is', 'playing', 'a', '.']\n",
      "a man is playing a \n",
      "['a', 'woman', 'is', 'playing', 'a', '.']\n",
      "a woman is playing a \n",
      "['someone', 'is', 'cutting', 'a', 'bowl', 'of', 'a', 'bowl', '.']\n",
      "someone is cutting a bowl of a bowl \n",
      "['a', 'man', 'is', 'playing', 'a', '.']\n",
      "a man is playing a \n",
      "['a', 'man', 'is', 'playing', 'a', '.']\n",
      "a man is playing a \n",
      "['a', 'man', 'is', 'playing', 'a', '.']\n",
      "a man is playing a \n",
      "['a', 'man', 'is', 'playing', 'a', 'a', '.']\n",
      "a man is playing a a \n",
      "['a', 'woman', 'is', 'playing', 'a', '.']\n",
      "a woman is playing a \n",
      "['a', 'man', 'is', 'playing', 'a', '.']\n",
      "a man is playing a \n",
      "['a', 'man', 'is', 'playing', '.']\n",
      "a man is playing \n",
      "['a', 'man', 'is', 'playing', 'a', 'large', '.']\n",
      "a man is playing a large \n",
      "['a', 'person', 'is', 'playing', 'a', '.']\n",
      "a person is playing a \n",
      "['a', 'woman', 'is', 'cutting', 'a', 'bowl', '.']\n",
      "a woman is cutting a bowl \n",
      "['a', 'person', 'is', 'playing', 'a', '.']\n",
      "a person is playing a \n",
      "['a', 'woman', 'is', 'playing', 'a', '.']\n",
      "a woman is playing a \n",
      "['a', 'person', 'is', 'playing', 'a', '.']\n",
      "a person is playing a \n",
      "['a', 'man', 'is', 'playing', 'a', '.']\n",
      "a man is playing a \n",
      "['a', 'man', 'is', 'playing', 'a', '.']\n",
      "a man is playing a \n",
      "['a', 'woman', 'is', 'cutting', 'a', 'bowl', '.']\n",
      "a woman is cutting a bowl \n",
      "['a', 'woman', 'is', 'cutting', 'a', '.']\n",
      "a woman is cutting a \n",
      "['a', 'man', 'is', 'playing', 'a', '.']\n",
      "a man is playing a \n",
      "['a', 'man', 'is', 'playing', 'a', '.']\n",
      "a man is playing a \n",
      "['a', 'woman', 'is', 'cutting', 'a', 'bowl', '.']\n",
      "a woman is cutting a bowl \n",
      "['a', 'man', 'is', 'playing', 'a', '.']\n",
      "a man is playing a \n",
      "['a', 'man', 'is', 'playing', 'a', '.']\n",
      "a man is playing a \n",
      "['a', 'man', 'is', 'playing', 'a', 'large', '.']\n",
      "a man is playing a large \n",
      "['a', 'boy', 'is', 'playing', '.']\n",
      "a boy is playing \n",
      "['a', 'woman', 'is', 'cutting', 'a', 'bowl', '.']\n",
      "a woman is cutting a bowl \n",
      "['two', 'man', 'is', 'playing', 'a', '.']\n",
      "two man is playing a \n",
      "['a', 'woman', 'is', 'cutting', 'a', '.']\n",
      "a woman is cutting a \n",
      "['a', 'man', 'is', 'playing', 'a', '.']\n",
      "a man is playing a \n",
      "['a', 'person', 'is', 'playing', 'a', '.']\n",
      "a person is playing a \n",
      "['someone', 'is', 'playing', 'a', 'into', 'of', '.']\n",
      "someone is playing a into of \n",
      "['a', 'woman', 'is', 'playing', 'a', '.']\n",
      "a woman is playing a \n",
      "['a', 'person', 'is', 'playing', '.']\n",
      "a person is playing \n",
      "['a', 'woman', 'is', 'playing', 'a', '.']\n",
      "a woman is playing a \n",
      "['a', 'woman', 'is', 'cutting', 'a', 'bowl', '.']\n",
      "a woman is cutting a bowl \n",
      "['a', 'man', 'is', 'playing', 'a', '.']\n",
      "a man is playing a \n",
      "['a', 'man', 'is', 'playing', 'a', '.']\n",
      "a man is playing a \n",
      "['a', 'man', 'is', 'playing', 'a', '.']\n",
      "a man is playing a \n",
      "['a', 'man', 'is', 'playing', 'a', '.']\n",
      "a man is playing a \n",
      "['a', 'woman', 'is', 'cutting', 'a', '.']\n",
      "a woman is cutting a \n",
      "['a', 'woman', 'is', 'playing', 'a', 'large', '.']\n",
      "a woman is playing a large \n",
      "['a', 'woman', 'is', 'cutting', 'a', '.']\n",
      "a woman is cutting a \n",
      "['a', 'man', 'is', 'playing', 'a', '.']\n",
      "a man is playing a \n",
      "['a', 'man', 'is', 'playing', 'a', '.']\n",
      "a man is playing a \n",
      "['a', 'woman', 'is', 'cutting', 'a', 'bowl', '.']\n",
      "a woman is cutting a bowl \n",
      "['a', 'woman', 'is', 'cutting', 'a', '.']\n",
      "a woman is cutting a \n",
      "['a', 'man', 'is', 'playing', 'a', '.']\n",
      "a man is playing a \n",
      "['a', 'woman', 'is', 'cutting', 'a', '.']\n",
      "a woman is cutting a \n",
      "['a', 'woman', 'is', 'playing', 'a', '.']\n",
      "a woman is playing a \n",
      "['a', 'woman', 'is', 'playing', '.']\n",
      "a woman is playing \n",
      "['a', 'man', 'is', 'playing', 'a', '.']\n",
      "a man is playing a \n",
      "['a', 'woman', 'is', 'playing', 'a', '.']\n",
      "a woman is playing a \n",
      "['someone', 'is', 'cutting', 'a', 'bowl', 'of', 'a', 'bowl', '.']\n",
      "someone is cutting a bowl of a bowl \n",
      "['a', 'person', 'is', 'playing', '.']\n",
      "a person is playing \n",
      "['a', 'woman', 'is', 'cutting', 'a', '.']\n",
      "a woman is cutting a \n",
      "['a', 'man', 'is', 'playing', 'a', '.']\n",
      "a man is playing a \n",
      "['a', 'man', 'is', 'playing', 'a', '.']\n",
      "a man is playing a \n",
      "['a', 'man', 'is', 'playing', 'a', '.']\n",
      "a man is playing a \n",
      "['a', 'woman', 'is', 'playing', 'a', '.']\n",
      "a woman is playing a \n",
      "['a', 'woman', 'is', 'playing', 'a', '.']\n",
      "a woman is playing a \n",
      "['a', 'woman', 'is', 'playing', 'a', '.']\n",
      "a woman is playing a \n",
      "['a', 'woman', 'is', 'cutting', 'a', 'bowl', '.']\n",
      "a woman is cutting a bowl \n",
      "['a', 'woman', 'is', 'playing', 'a', '.']\n",
      "a woman is playing a \n",
      "['a', 'woman', 'is', 'playing', 'a', '.']\n",
      "a woman is playing a \n",
      "['a', 'woman', 'is', 'cutting', 'a', 'bowl', '.']\n",
      "a woman is cutting a bowl \n",
      "['a', 'person', 'playing', 'a', 'man', '.']\n",
      "a person playing a man \n",
      "['a', 'man', 'is', 'playing', 'a', '.']\n",
      "a man is playing a \n",
      "['a', 'man', 'is', 'playing', '.']\n",
      "a man is playing \n",
      "['a', 'woman', 'is', 'playing', 'a', '.']\n",
      "a woman is playing a \n",
      "['a', 'woman', 'is', 'cutting', 'a', '.']\n",
      "a woman is cutting a \n",
      "['a', 'man', 'is', 'playing', 'a', '.']\n",
      "a man is playing a \n",
      "['a', 'person', 'is', 'playing', 'a', '.']\n",
      "a person is playing a \n",
      "['a', 'woman', 'is', 'playing', 'a', '.']\n",
      "a woman is playing a \n",
      "['a', 'person', 'is', 'playing', '.']\n",
      "a person is playing \n",
      "['a', 'woman', 'is', 'playing', 'with', 'a', '.']\n",
      "a woman is playing with a \n",
      "['someone', 'is', 'cutting', 'a', 'bowl', 'of', 'a', 'bowl', '.']\n",
      "someone is cutting a bowl of a bowl \n",
      "['a', 'person', 'is', 'playing', 'a', '.']\n",
      "a person is playing a \n",
      "['a', 'man', 'is', 'playing', 'a', '.']\n",
      "a man is playing a \n",
      "['a', 'man', 'is', 'playing', 'a', '.']\n",
      "a man is playing a \n",
      "['a', 'woman', 'is', 'cutting', 'a', '.']\n",
      "a woman is cutting a \n",
      "['a', 'person', 'is', 'playing', 'a', '.']\n",
      "a person is playing a \n",
      "['a', 'man', 'is', 'playing', 'a', '.']\n",
      "a man is playing a \n",
      "['two', 'are', 'playing', '.']\n",
      "two are playing \n",
      "['a', 'person', 'is', 'playing', 'a', '.']\n",
      "a person is playing a \n",
      "['a', 'woman', 'is', 'playing', 'a', '.']\n",
      "a woman is playing a \n",
      "['a', 'woman', 'is', 'cutting', 'a', '.']\n",
      "a woman is cutting a \n"
     ]
    }
   ],
   "source": [
    "#============to json============#\n",
    "predict_label = []\n",
    "with open('predict_label.txt', 'w') as f:\n",
    "    for video_name, _id in video_id_t.items():\n",
    "        tokens = OUTPUTS[int(_id)]\n",
    "        predict = \" \".join(tokens)\n",
    "        \n",
    "        if(predict[-1] == \".\"):\n",
    "            predict  = predict[:-1]\n",
    "        #print(OUTPUTS[int(_id)])\n",
    "        #input(\"\")\n",
    "        f.write(str(video_name) + \",\" + predict +\"\\n\")\n",
    "        print(predict)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "#video_id_t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "os.system(\"python bleu_eval.py predict_label.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
