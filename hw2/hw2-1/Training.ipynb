{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.layers import Input, Softmax, Dense, Dropout\n",
    "from tensorflow.keras.layers import CuDNNGRU, CuDNNLSTM\n",
    "from tensorflow.keras.layers import Bidirectional, TimeDistributed, Concatenate\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "import numpy as np\n",
    "import json\n",
    "import nltk\n",
    "from nltk import word_tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "idfilename = 'training_data/id.txt'\n",
    "datadirname = 'training_data/feat/'\n",
    "labelfilename = 'training_label.json'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#load dictionary\n",
    "\n",
    "with open(\"DIC_word_index.json\") as f:\n",
    "    DIC_word_index = json.load(f)\n",
    "    \n",
    "with open(\"DIC_index_word.json\") as f:\n",
    "    DIC_index_word = json.load(f)\n",
    "\n",
    "#DIC_index_word = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: implement Sent2Seq\n",
    "# hyperparameter: min count > 3 (discard terms with freq <= 3)\n",
    "def Sent2Seq(sent):\n",
    "#     print(sent)\n",
    "    tokens = word_tokenize(sent.lower())\n",
    "    ret = []\n",
    "#     print(tokens)\n",
    "    for word in tokens:\n",
    "        #print(word)\n",
    "        ret.append(DIC_word_index[word])\n",
    "    #input(\"\")\n",
    "    return ret"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1450/1450 [00:00<00:00, 6649.71it/s]\n"
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "#add a space for split function easy to handle\n",
    "BOS = \"<bos>\"\n",
    "EOS = \"<eos>\"\n",
    "PAD = \"<pad>\"\n",
    "UNK = \"<unk>\"\n",
    "\n",
    "BOS_index = DIC_word_index[BOS]\n",
    "EOS_index = DIC_word_index[EOS]\n",
    "\n",
    "# loading training data\n",
    "\n",
    "video_feat = {}\n",
    "video_id = {}\n",
    "for i,lb in enumerate(open(idfilename)):\n",
    "    #lb contains '\\n', therefore lb[:-1]\n",
    "    lb = lb[:-1]\n",
    "    video_feat[lb] = np.load(datadirname + lb + \".npy\")\n",
    "    video_id[lb] = i\n",
    "    \n",
    "# TRAIN_SZ = len(encode_x)\n",
    "# decode_x = [[]]*TRAIN_SZ\n",
    "# decode_y = [[]]*TRAIN_SZ\n",
    "\n",
    "sampling = 1\n",
    "decode_x = []\n",
    "decode_y = []\n",
    "encode_x = []\n",
    "MAX_SEQ_LEN = 10;\n",
    "\n",
    "# loading decoder data\n",
    "rawlabels = json.load(open(labelfilename, 'r'))\n",
    "for data in tqdm(rawlabels):\n",
    "    vid = data['id']\n",
    "    index = video_id[vid]\n",
    "#     print(index)\n",
    "    for k in range(sampling):\n",
    "        sent =  data['caption'][k]\n",
    "        sent_seq = Sent2Seq(sent)\n",
    "        decode_x.append([BOS_index] + sent_seq)\n",
    "        decode_y.append(sent_seq + [EOS_index])\n",
    "        encode_x.append(video_feat[vid])\n",
    "#         if(len(video_feat[vid]) > MAX_SEQ_LEN):\n",
    "#             MAX_SEQ_LEN = len(video_feat[vid])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "del video_id\n",
    "del video_feat\n",
    "del rawlabels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "VOCAB_SZ = len(DIC_word_index) # maybe? need statistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1450\n",
      "8\n",
      "MAX_X_LEN:10\n",
      "(1450, 10, 6087)\n",
      "(1450, 10, 1)\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "# data1 = pad_sequences(data1, maxlen=MAX_IN_LEN, padding='pre', truncating='pre')\n",
    "\n",
    "#decode_x will be the same len as decode_y\n",
    "print(len(decode_x))\n",
    "print(len(decode_x[0]))\n",
    "decode_x = pad_sequences(decode_x, maxlen=MAX_SEQ_LEN, padding='post', truncating='post')\n",
    "decode_y = pad_sequences(decode_y, maxlen=MAX_SEQ_LEN, padding='post', truncating='post')\n",
    "\n",
    "#print(decode_x)\n",
    "#print(decode_y)\n",
    "print(\"MAX_X_LEN:%d\"%(MAX_SEQ_LEN))\n",
    "# decode_x = decode_x.reshape(decode_x.shape[0],decode_x.shape[1], 1)\n",
    "decode_y = decode_y.reshape(decode_y.shape[0],decode_y.shape[1], 1)\n",
    "decode_x = to_categorical(decode_x, num_classes=VOCAB_SZ)\n",
    "# decode_y = to_categorical(decode_y, num_classes=VOCAB_SZ)\n",
    "print(decode_x.shape)\n",
    "print(decode_y.shape)\n",
    "# print(TRAIN_SZ)\n",
    "#input(\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "EncoderInput (InputLayer)       (None, 80, 4096)     0                                            \n",
      "__________________________________________________________________________________________________\n",
      "Encoder (CuDNNGRU)              (None, 128)          1622784     EncoderInput[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "DecoderInput (InputLayer)       (None, 10, 6087)     0                                            \n",
      "__________________________________________________________________________________________________\n",
      "repeat_vector (RepeatVector)    (None, 10, 128)      0           Encoder[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "Interaction (Concatenate)       (None, 10, 6215)     0           DecoderInput[0][0]               \n",
      "                                                                 repeat_vector[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "Decoder (CuDNNGRU)              (None, 10, 128)      2436480     Interaction[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "time_distributed (TimeDistribut (None, 10, 6087)     785223      Decoder[0][0]                    \n",
      "==================================================================================================\n",
      "Total params: 4,844,487\n",
      "Trainable params: 4,844,487\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.layers import Input, Softmax, Dense, Dropout\n",
    "from tensorflow.keras.layers import CuDNNGRU, CuDNNLSTM\n",
    "from tensorflow.keras.layers import Bidirectional, TimeDistributed, Concatenate, RepeatVector\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "\n",
    "EncoderDIM = 128\n",
    "DecoderDIM = 128\n",
    "\n",
    "#decode_x = decode_x[:MAX_SEQ_LEN,:]\n",
    "#decode_y = decode_y[:MAX_SEQ_LEN,:]\n",
    "\n",
    "# Layers\n",
    "t_encoder_input = Input(shape=(80, 4096), name=\"EncoderInput\")\n",
    "L_encoder = CuDNNGRU(EncoderDIM, name='Encoder')\n",
    "L_Repeat = RepeatVector(MAX_SEQ_LEN)\n",
    "\n",
    "t_decoder_input = Input(shape=(MAX_SEQ_LEN,VOCAB_SZ), name=\"DecoderInput\")\n",
    "L_Concat = Concatenate(axis=-1, name=\"Interaction\")\n",
    "L_decoder = CuDNNGRU(DecoderDIM, return_sequences=True, name='Decoder')\n",
    "L_Dense = Dense(VOCAB_SZ, activation='softmax', name=\"Dense\")\n",
    "\n",
    "# tensors\n",
    "t_encoder_outputs = L_encoder(t_encoder_input)\n",
    "t_encoder_outputs = L_Repeat(t_encoder_outputs)\n",
    "\n",
    "t_decoder_mod_input = L_Concat([t_decoder_input, t_encoder_outputs])\n",
    "\n",
    "t_decoder_outputs = L_decoder(t_decoder_mod_input)\n",
    "t_out_probs = TimeDistributed(L_Dense)(t_decoder_outputs)\n",
    "\n",
    "\n",
    "model = Model(inputs=[t_encoder_input, t_decoder_input], outputs=t_out_probs)\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from tensorflow.keras.callbacks import EarlyStopping,ReduceLROnPlateau\n",
    "optimizer = Adam(lr=1e-3)\n",
    "model.compile(optimizer=optimizer, loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# model.compile(optimizer=optimizer, loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "#  earlystp = EarlyStopping(monitor=\"loss\", patience=20, verbose=1, mode=\"auto\")\n",
    "# checkpoint = ModelCheckpoint(model_name+'_{epoch:02d}.hdf5', monitor='val_loss', \\\n",
    "#                              verbose=0, save_best_only=True, save_weights_only=False, \\\n",
    "#                              mode='auto', period=1)\n",
    "lrreduc = ReduceLROnPlateau(monitor='loss', factor=0.5,\\\n",
    "                             patience=5, min_lr=0.00001, verbose=1, cooldown=5)\n",
    "\n",
    "model.fit(x=[encode_x, decode_x], y=decode_y, batch_size=4, epochs=200, callbacks=[lrreduc])\n",
    "mdname = 'modelv3.h5'\n",
    "model.save_weights(mdname)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mdname = 'modelv3.h5'\n",
    "model.load_weights(mdname)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.layers import Input, Softmax, Dense, Dropout\n",
    "from tensorflow.keras.layers import CuDNNGRU, CuDNNLSTM\n",
    "from tensorflow.keras.layers import Bidirectional, TimeDistributed, Concatenate, RepeatVector\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.optimizers import Adam"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "EncoderInput (InputLayer)    (None, 80, 4096)          0         \n",
      "_________________________________________________________________\n",
      "Encoder (CuDNNGRU)           (None, 128)               1622784   \n",
      "=================================================================\n",
      "Total params: 1,622,784\n",
      "Trainable params: 1,622,784\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "EncoderDIM = 128\n",
    "DecoderDIM = 128\n",
    "mdname = 'modelv3.h5'\n",
    "\n",
    "# Encoder model\n",
    "# Layers\n",
    "t_encoder_input = Input(shape=(80, 4096), name=\"EncoderInput\")\n",
    "L_encoder = CuDNNGRU(EncoderDIM, name='Encoder')\n",
    "# tensors\n",
    "t_encoder_outputs = L_encoder(t_encoder_input)\n",
    "emodel = Model(inputs=t_encoder_input, outputs=t_encoder_outputs)\n",
    "emodel.summary()\n",
    "emodel.load_weights(mdname, by_name=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "DecoderInput (InputLayer)       (None, 1, 6087)      0                                            \n",
      "__________________________________________________________________________________________________\n",
      "DecoderEncodedInput (InputLayer (None, 1, 128)       0                                            \n",
      "__________________________________________________________________________________________________\n",
      "Interaction (Concatenate)       (None, 1, 6215)      0           DecoderInput[0][0]               \n",
      "                                                                 DecoderEncodedInput[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "DecoderStateInput (InputLayer)  (None, 128)          0                                            \n",
      "__________________________________________________________________________________________________\n",
      "Decoder (CuDNNGRU)              [(None, 128), (None, 2436480     Interaction[0][0]                \n",
      "                                                                 DecoderStateInput[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "Dense (Dense)                   (None, 6087)         785223      Decoder[0][0]                    \n",
      "==================================================================================================\n",
      "Total params: 3,221,703\n",
      "Trainable params: 3,221,703\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# Decoder model\n",
    "# Layers\n",
    "t_decoder_input = Input(shape=(1,VOCAB_SZ), name=\"DecoderInput\")\n",
    "t_decoder_enc_input = Input(shape=(1,EncoderDIM), name=\"DecoderEncodedInput\")\n",
    "t_decoder_state_input = Input(shape=(DecoderDIM,), name=\"DecoderStateInput\")\n",
    "\n",
    "L_Concat = Concatenate(axis=-1, name=\"Interaction\")\n",
    "L_decoder = CuDNNGRU(DecoderDIM, return_state=True, name='Decoder')\n",
    "# L_Dense = Dense(VOCAB_SZ, activation='softmax', name=\"Dense\")\n",
    "\n",
    "# tensors\n",
    "t_decoder_mod_input = L_Concat([t_decoder_input, t_decoder_enc_input])\n",
    "\n",
    "t_decoder_outputs, t_state = L_decoder(t_decoder_mod_input, initial_state=t_decoder_state_input)\n",
    "t_out_probs = L_Dense(t_decoder_outputs)\n",
    "\n",
    "\n",
    "dmodel = Model(inputs=[t_decoder_input, t_decoder_enc_input, t_decoder_state_input], outputs=[t_out_probs, t_state])\n",
    "dmodel.summary()\n",
    "dmodel.load_weights(mdname, by_name=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "\n",
    "BOS = '<bos>'\n",
    "EOS = '<eos>'\n",
    "\n",
    "def decode_sequence(input_seq):\n",
    "    # Encode the input as state vectors.\n",
    "    encoded_value = emodel.predict(input_seq)\n",
    "#     encoded_value = np.expand_dims(encoded_value, axis=1)\n",
    "    \n",
    "    feed_seq = DIC_word_index[BOS]\n",
    "    feed_state = np.zeros((1,DecoderDIM))\n",
    "\n",
    "    stop_condition = False\n",
    "    decoded_sentence = []\n",
    "    t = 0\n",
    "    while not stop_condition:\n",
    "        feed_seq_cat = to_categorical([feed_seq], num_classes=VOCAB_SZ)\n",
    "        output_token, h = dmodel.predict([[feed_seq_cat], [encoded_value], feed_state])\n",
    "#         output_token, h = dmodel.predict([[feed_seq_cat], encoded_value, feed_state])\n",
    "\n",
    "        # Sample a token\n",
    "        sampled_token_index = np.argmax(output_token[0, :])\n",
    "        sampled_word = DIC_index_word[str(sampled_token_index)]\n",
    "        decoded_sentence.append(sampled_word)\n",
    "        print(sampled_word)\n",
    "\n",
    "        # Exit condition: either hit max length\n",
    "        # or find stop character.\n",
    "        if (sampled_word == EOS or len(decoded_sentence) >= MAX_SEQ_LEN):\n",
    "            stop_condition = True\n",
    "        else:\n",
    "            # Update the target sequence (of length 1).\n",
    "            t += 1\n",
    "            feed_seq = sampled_token_index\n",
    "\n",
    "            # Update states\n",
    "            feed_state = h\n",
    "\n",
    "    return decoded_sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "a\n",
      "man\n",
      "is\n",
      "playing\n",
      "the\n",
      "guitar\n",
      ".\n",
      "<eos>\n",
      "1\n"
     ]
    }
   ],
   "source": [
    "indexx = 32\n",
    "test = encode_x[indexx].reshape(1, encode_x[indexx].shape[0],encode_x[indexx].shape[1])\n",
    "decode_sequence(test)\n",
    "print(len(test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
