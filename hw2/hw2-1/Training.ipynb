{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.layers import Input, Softmax, Dense, Dropout\n",
    "from tensorflow.keras.layers import CuDNNGRU, CuDNNLSTM\n",
    "from tensorflow.keras.layers import Bidirectional, TimeDistributed\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "import numpy as np\n",
    "import json\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "idfilename = 'training_data/id.txt'\n",
    "datadirname = 'training_data/feat/'\n",
    "labelfilename = 'training_label.json'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#load dictionary\n",
    "\n",
    "with open(\"DIC_word_index.json\") as f:\n",
    "    DIC_word_index = json.load(f)\n",
    "    \n",
    "with open(\"DIC_index_word.json\") as f:\n",
    "    DIC_index_word = json.load(f)\n",
    "\n",
    "#DIC_index_word = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: implement Sent2Seq\n",
    "# hyperparameter: min count > 3 (discard terms with freq <= 3)\n",
    "def Sent2Seq(sent):\n",
    "    print(sent)\n",
    "    sent = re.sub(r'[^\\w\\s\\<\\>\\-]','',sent)\n",
    "    tokens = sent.lower().split()\n",
    "    ret = []\n",
    "    print(tokens)\n",
    "    for word in tokens:\n",
    "        #print(word)\n",
    "        ret.append(DIC_word_index[word])\n",
    "    #input(\"\")\n",
    "    return ret"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#add a space for split function easy to handle\n",
    "BOS = \"<bos> \"\n",
    "EOS = \" <eos>\"\n",
    "\n",
    "# loading training data\n",
    "encode_x = []\n",
    "video_id = {}\n",
    "for i,lb in enumerate(open(idfilename)):\n",
    "    #lb contains '\\n', therefore lb[:-1]\n",
    "    lb = lb[:-1]\n",
    "    x = np.load(datadirname + lb + \".npy\")\n",
    "    encode_x.append(x)\n",
    "    video_id[lb] = i\n",
    "    \n",
    "TRAIN_SZ = len(encode_x)\n",
    "decode_x = [[]]*TRAIN_SZ\n",
    "decode_y = [[]]*TRAIN_SZ\n",
    "\n",
    "MAX_SEQ_LEN = 0;\n",
    "\n",
    "# loading decoder data\n",
    "rawlabels = json.load(open(labelfilename, 'r'))\n",
    "for data in rawlabels:\n",
    "    \n",
    "    index = video_id[data['id']]\n",
    "    print(index)\n",
    "    sent =  data['caption'][0] # select one sentence for now\n",
    "    # TODO: implement Sent2Seq\n",
    "    decode_x[index] = Sent2Seq(BOS+sent)\n",
    "    decode_y[index] = Sent2Seq(sent+EOS)\n",
    "    if(len(decode_x[index]) > MAX_SEQ_LEN):\n",
    "        MAX_SEQ_LEN = len(decode_x[index])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "VOCAB_SZ = len(DIC_word_index) # maybe? need statistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "# data1 = pad_sequences(data1, maxlen=MAX_IN_LEN, padding='pre', truncating='pre')\n",
    "\n",
    "#decode_x will be the same len as decode_y\n",
    "print(len(decode_x))\n",
    "print(len(decode_x[0]))\n",
    "decode_x = pad_sequences(decode_x, maxlen=MAX_SEQ_LEN, padding='pre', truncating='pre')\n",
    "decode_y = pad_sequences(decode_y, maxlen=MAX_SEQ_LEN, padding='pre', truncating='pre')\n",
    "\n",
    "#print(decode_x)\n",
    "#print(decode_y)\n",
    "print(\"MAX_X_LEN:%d\"%(MAX_SEQ_LEN))\n",
    "# decode_x = decode_x.reshape(decode_x.shape[0],decode_x.shape[1], 1)\n",
    "decode_y = decode_y.reshape(decode_y.shape[0],decode_y.shape[1], 1)\n",
    "decode_x = to_categorical(decode_x, num_classes=VOCAB_SZ)\n",
    "# decode_y = to_categorical(decode_y, num_classes=VOCAB_SZ)\n",
    "print(decode_x.shape)\n",
    "print(decode_y.shape)\n",
    "print(TRAIN_SZ)\n",
    "#input(\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Using sparse_categorical_crossentropy, we only need to pass integers as input to decoder.\n",
    "EncoderDIM = 256\n",
    "DecoderDIM = 256\n",
    "\n",
    "#decode_x = decode_x[:MAX_SEQ_LEN,:]\n",
    "#decode_y = decode_y[:MAX_SEQ_LEN,:]\n",
    "\n",
    "# Layers\n",
    "t_encoder_input = Input(shape=(80, 4096), name=\"EncoderInput\")\n",
    "t_decoder_input = Input(shape=(MAX_SEQ_LEN,VOCAB_SZ), name=\"DecoderInput\")\n",
    "L_encoder = CuDNNGRU(EncoderDIM, return_state=True, name='Encoder')\n",
    "L_decoder = CuDNNGRU(DecoderDIM, return_sequences=True, return_state=True, name='Decoder')\n",
    "L_Dense = Dense(VOCAB_SZ, name=\"Dense\", activation='softmax')\n",
    "\n",
    "# tensors\n",
    "t_encoder_outputs, state_h = L_encoder(t_encoder_input)\n",
    "t_decoder_outputs, _ = L_decoder(t_decoder_input, initial_state=state_h)\n",
    "t_out_probs = TimeDistributed(L_Dense)(t_decoder_outputs)\n",
    "\n",
    "\n",
    "model = Model(inputs=[t_encoder_input, t_decoder_input], outputs=t_out_probs)\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "optimizer = Adam(lr=1e-3)\n",
    "model.compile(optimizer=optimizer, loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
    "# model.compile(optimizer=optimizer, loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "earlystp = EarlyStopping(monitor=\"loss\", patience=20, verbose=1, mode=\"auto\")\n",
    "checkpoint = ModelCheckpoint(model_name+'_{epoch:02d}.hdf5', monitor='val_loss', \\\n",
    "                             verbose=0, save_best_only=True, save_weights_only=False, \\\n",
    "                             mode='auto', period=1)\n",
    "lrreduc = ReduceLROnPlateau(monitor='loss', factor=0.5,\n",
    "                              patience=5, min_lr=0.00001, verbose=1, cooldown=5)\n",
    "\n",
    "model.fit(x=[encode_x, decode_x], y=decode_y, batch_size=1, epochs=200) #callbacks=[]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save_weights('modelv1.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "EncoderDIM = 128\n",
    "DecoderDIM = 128\n",
    "\n",
    "# Encoder model\n",
    "# Layers\n",
    "t_encoder_input = Input(shape=(80, 4096), name=\"EncoderInput\")\n",
    "L_encoder = CuDNNGRU(EncoderDIM, return_state=True, name='Encoder')\n",
    "# tensors\n",
    "t_encoder_outputs, state_h = L_encoder(t_encoder_input)\n",
    "emodel = Model(inputs=t_encoder_input, outputs=state_h)\n",
    "emodel.summary()\n",
    "emodel.load_weights('modelv1.h5', by_name=True)\n",
    "\n",
    "\n",
    "# Decoder model\n",
    "# Layers\n",
    "t_decoder_input = Input(shape=(MAX_SEQ_LEN,VOCAB_SZ), name=\"DecoderInput\")\n",
    "t_decoder_state_input = Input(shape=(DecoderDIM,), name=\"DecoderStateInput\")\n",
    "L_decoder = CuDNNGRU(DecoderDIM, return_sequences=True, return_state=True, name='Decoder')\n",
    "L_Dense = Dense(VOCAB_SZ, name=\"Dense\", activation='softmax')\n",
    "\n",
    "# tensors\n",
    "t_decoder_outputs, t_decode_state = L_decoder(t_decoder_input, initial_state=t_decoder_state_input)\n",
    "t_out_probs = TimeDistributed(L_Dense)(t_decoder_outputs)\n",
    "\n",
    "\n",
    "dmodel = Model(inputs=[t_decoder_input, t_decoder_state_input], outputs=[t_out_probs, t_decode_state])\n",
    "dmodel.summary()\n",
    "dmodel.load_weights('modelv1.h5', by_name=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "\n",
    "BOS = '<bos>'\n",
    "EOS = '<eos>'\n",
    "\n",
    "def decode_sequence(input_seq):\n",
    "    # Encode the input as state vectors.\n",
    "    states_value = emodel.predict(input_seq)\n",
    "\n",
    "    # Generate empty target sequence of length 1.\n",
    "    target_seq = pad_sequences([[ DIC_word_index[BOS] ]], maxlen=MAX_SEQ_LEN, padding='post', truncating='post')\n",
    "    \n",
    "\n",
    "    # Sampling loop for a batch of sequences\n",
    "    # (to simplify, here we assume a batch of size 1).\n",
    "    stop_condition = False\n",
    "    decoded_sentence = []\n",
    "    t = 0\n",
    "    while not stop_condition:\n",
    "        target_seq_cat = to_categorical(target_seq, num_classes=VOCAB_SZ)\n",
    "\n",
    "        output_tokens, h = dmodel.predict([target_seq_cat, states_value])\n",
    "\n",
    "        # Sample a token\n",
    "        sampled_token_index = np.argmax(output_tokens[0, -1, :])\n",
    "#         sampled_token_index = np.argmax(output_tokens[0, 0, :])\n",
    "        sampled_word = DIC_index_word[str(sampled_token_index)]\n",
    "        decoded_sentence.append(sampled_word)\n",
    "        print(decoded_sentence)\n",
    "\n",
    "        # Exit condition: either hit max length\n",
    "        # or find stop character.\n",
    "        if (sampled_word == EOS or len(decoded_sentence) >= MAX_SEQ_LEN):\n",
    "            stop_condition = True\n",
    "        else:\n",
    "            # Update the target sequence (of length 1).\n",
    "            t += 1\n",
    "            target_seq[0][t] = sampled_token_index\n",
    "\n",
    "            # Update states\n",
    "            states_value = h\n",
    "\n",
    "    return decoded_sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test = encode_x[0].reshape(1, encode_x[0].shape[0],encode_x[0].shape[1])\n",
    "decode_sequence(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "i = DIC_word_index['amanplaysaguitar']\n",
    "DIC_index_word[str(i)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
