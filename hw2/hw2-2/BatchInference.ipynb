{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import os\n",
    "# os.environ[\"CUDA_VISIBLE_DEVICES\"] =\"-1\"\n",
    "import torch\n",
    "import numpy as np\n",
    "import torch.nn as nn\n",
    "# import torch.nn.functional as F\n",
    "# import torchvision\n",
    "\n",
    "#from keras.preprocessing.sequence import pad_sequences\n",
    "#from keras.utils import to_categorical\n",
    "\n",
    "from tqdm import tqdm_notebook as tqdm\n",
    "\n",
    "import json\n",
    "import nltk\n",
    "from nltk import word_tokenize\n",
    "\n",
    "from torch.utils.data.dataloader import default_collate\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(device)\n",
    "DIC_word_index = json.load(open(\"vocab.json\", \"r\", encoding='utf-8'))\n",
    "DIC_index_word = {index:word for word, index in DIC_word_index.items()}\n",
    "word_vectors = np.load(\"wv_matrix100d.npy\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# if you don't have money, use this cell\n",
    "\n",
    "# device = torch.device(\"cpu\")\n",
    "# print(device)\n",
    "# DIC_word_index = json.load(open(\"vocab.json\", \"r\", encoding='utf-8'))\n",
    "# DIC_index_word = {index:word for word, index in DIC_word_index.items()}\n",
    "# word_vectors = np.load(\"wv_matrix100d.npy\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "sent_len = 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [],
   "source": [
    "class InferenceAttModel(torch.nn.Module):\n",
    "    def __init__(self, latent_dim, seq_length, embed_dim, word_vectors):\n",
    "        super(InferenceAttModel, self).__init__()\n",
    "        \n",
    "        self.hidden_dim = latent_dim        \n",
    "        self.seq_length = seq_length\n",
    "        self.embed_dim = embed_dim\n",
    "        \n",
    "#         self.embedding = torch.nn.Embedding(len(word_vectors), embed_dim)\n",
    "#         self.embedding.load_state_dict({'weight': word_vectors})\n",
    "#         self.embedding.weight.requires_grad = False\n",
    "        self.embedding = torch.nn.Embedding.from_pretrained(torch.from_numpy(word_vectors), freeze=True)\n",
    "#        self.embedding = torch.nn.Embedding(len(word_vectors), embed_dim)\n",
    "    \n",
    "        self.soft = torch.nn.Softmax(-2)\n",
    "        \n",
    "        self.RNN1 = torch.nn.LSTM(self.embed_dim, latent_dim,  num_layers=2, batch_first = True)\n",
    "        self.RNN2 = torch.nn.LSTM(self.embed_dim, latent_dim,  num_layers=2, batch_first = True)\n",
    "        \n",
    "        self.Projection_layer = torch.nn.Sequential(\n",
    "            torch.nn.Linear(2 * latent_dim, len(word_vectors)),\n",
    "        )\n",
    "        \n",
    "        self.trainable_W = torch.nn.Sequential(\n",
    "            torch.nn.Linear(latent_dim ,  latent_dim),\n",
    "            torch.nn.Sigmoid(),\n",
    "            torch.nn.Dropout(p=0.2),\n",
    "        )\n",
    "    def encoder_RNN(self, e_x):\n",
    "        e_y, hiddens = self.RNN1(e_x)\n",
    "        \n",
    "        return e_y, hiddens\n",
    "    \n",
    "    def decoder_RNN(self, d_x, hiddens):\n",
    "        d_y, hiddens2 = self.RNN2(d_x, hiddens)\n",
    "        return d_y, hiddens2\n",
    "    \n",
    "    def forward(self, e_x, ):\n",
    "        e_x = self.embedding(e_x).float()\n",
    "        batch_size = e_x.shape[0]\n",
    "        \"\"\"\n",
    "        if(batch_size != 1):\n",
    "            print(batch_size)\n",
    "            print(\"Batch size > 1 is not available now. Batch size please give 1.\")\n",
    "            return\n",
    "        \"\"\"\n",
    "        #print(sent.shape) torch.Size([2, 15, 6087])\n",
    "\n",
    "        e_output, hiddens = self.encoder_RNN(e_x)\n",
    "        #attention\n",
    "        uW = self.trainable_W(e_output)\n",
    "        \n",
    "        first = True\n",
    "\n",
    "        first_index = DIC_word_index['<bos>']\n",
    "        first_input = torch.tensor([[first_index] * batch_size]).to(device)\n",
    "        first_input = first_input.view(batch_size, 1, 1)\n",
    "        for i in range(self.seq_length):\n",
    "            #embed_dim default:100\n",
    "            #seq_length = sent_len, default:5\n",
    "            if(first):\n",
    "                word = first_input;\n",
    "\n",
    "                \n",
    "            #word.shape: [batch_size, 1, 1]\n",
    "            word = self.embedding(word.view(1, -1)).float();\n",
    "            #word.shape: [1, batch_size, embed_dim]\n",
    "            #one for each word, therefore d_output = d_state\n",
    "            \n",
    "            \n",
    "            d_output, hiddens = self.decoder_RNN(word.transpose(1,0), hiddens)\n",
    "            #d_output.shape: [batch_size, 1, hidden_dim]\n",
    "            uWv = torch.matmul(uW, d_output.transpose(2,1))\n",
    "            #uWv.shape: [batch_size, seq_length, 1]\n",
    "            alpha = self.soft(uWv)\n",
    "\n",
    "            new_context = e_output.transpose(2,1) @ alpha;\n",
    "            #e_output.shape [batch_size, seq_length, hidden_dim]\n",
    "            #alpha.shape [batch_size, seq_length, 1]\n",
    "            \n",
    "            #new_context.shape [batch_size, hidden_dim, 1]          \n",
    "            pre = torch.cat((d_output, new_context.transpose(2,1)), 2)\n",
    "            ans_logits = self.Projection_layer(pre)\n",
    "            #ans_index = np.argmax(ans_logits.cpu().detach().numpy())\n",
    "            \n",
    "            values, indices = torch.max(ans_logits, -1)\n",
    "            word = indices;\n",
    "            #word.shape [batch_size, 1]\n",
    "            if(first):\n",
    "                batch_seq = word;\n",
    "                first = False\n",
    "            else:\n",
    "                batch_seq = torch.cat((batch_seq, word), 1)\n",
    "            \n",
    "            #ans = DIC_index_word[ans_index]\n",
    "            #ans_seq.append(ans)\n",
    "            #word = torch.tensor([ans_index])\n",
    "        \n",
    "        #logits = torch.stack(logits, 1)\n",
    "        return batch_seq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [],
   "source": [
    "#\n",
    "def Inference(model_path, question_path):\n",
    "    def pad_post_zero(a, length):\n",
    "        ret = []\n",
    "        for _list in a:\n",
    "            if(len(_list) < length):\n",
    "                for ct in range(len(_list),length,1):\n",
    "                    _list.append(DIC_word_index[\"<pad>\"])\n",
    "            if(len(_list) > length):\n",
    "                _list = _list[:length]\n",
    "\n",
    "            ret.append(_list)\n",
    "        return ret\n",
    "    def my_collate(datas):\n",
    "        batch = {}\n",
    "        batch['encoder_x'] = torch.tensor([data for data in datas])\n",
    "        return batch\n",
    "    latent_dim = 1024\n",
    "    embed_dim = word_vectors.shape[1]\n",
    "    batch_size = 64\n",
    "    \n",
    "    model = InferenceAttModel(latent_dim, sent_len, embed_dim, word_vectors)\n",
    "    #saved_model = torch.load(\"models/Model92\")\n",
    "    saved_model = torch.load(model_path)\n",
    "    model.load_state_dict(saved_model['model'])\n",
    "    model = model.to(device)\n",
    "    model.eval()\n",
    "    \n",
    "    encode_x = []\n",
    "    #with open('sel_conversation/question.txt', 'r') as f:\n",
    "    with open(question_path, 'r') as f:\n",
    "        for l in f:\n",
    "            l = l.split()\n",
    "            tmp = []\n",
    "            for word in l:\n",
    "                try:\n",
    "                    index = DIC_word_index[word]\n",
    "                except KeyError:\n",
    "                    index = DIC_word_index['<unk>']\n",
    "                tmp.append(index)\n",
    "            encode_x.append([DIC_word_index['<bos>']] + tmp + [DIC_word_index['<eos>']])        \n",
    "    encode_x = pad_post_zero(encode_x, sent_len)\n",
    "    \n",
    "    #print(np.array(encode_x).shape); (len(encode_x, sent_len)\n",
    "    dataloader = torch.utils.data.DataLoader(encode_x, batch_size = batch_size, shuffle = False, collate_fn = my_collate)\n",
    "    ANSWERS = list()\n",
    "    for i, batch in tqdm(enumerate(dataloader), total=len(dataloader)):\n",
    "        with torch.no_grad():\n",
    "            e_x = batch['encoder_x'].long().to(device)\n",
    "        batch_seq = model.forward(e_x)\n",
    "        for seq in batch_seq:\n",
    "            #seq shape: [sent_len]\n",
    "            ans = []\n",
    "            for ans_index in seq:\n",
    "                ans.append(DIC_index_word[ans_index.item()])\n",
    "            ANSWERS.append(ans)\n",
    "    return(ANSWERS)\n",
    "    \"\"\"\n",
    "    ANSWERS = list()\n",
    "    for index in range(5):\n",
    "        e_x = torch.tensor([encode_x[index]]).long().to(device)\n",
    "        ans = model.forward(e_x)\n",
    "        ANSWERS.append(ans)\n",
    "    \n",
    "    return(ANSWERS)\n",
    "    \"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cd4ef5f52e794ff19441d91f736efa58",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=13760), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-114-4fe77ff325a2>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mANSWERS\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mInference\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel_path\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"models/Model1\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mquestion_path\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'sel_conversation/question.txt'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-113-4668e744a7a9>\u001b[0m in \u001b[0;36mInference\u001b[0;34m(model_path, question_path)\u001b[0m\n\u001b[1;32m     48\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mno_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     49\u001b[0m             \u001b[0me_x\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'encoder_x'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlong\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 50\u001b[0;31m         \u001b[0mbatch_seq\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me_x\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     51\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mseq\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mbatch_seq\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     52\u001b[0m             \u001b[0;31m#seq shape: [sent_len]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-108-84c55347261a>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, e_x)\u001b[0m\n\u001b[1;32m     69\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     70\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 71\u001b[0;31m             \u001b[0md_output\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhiddens\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdecoder_RNN\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mword\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtranspose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhiddens\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     72\u001b[0m             \u001b[0;31m#d_output.shape: [batch_size, 1, hidden_dim]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     73\u001b[0m             \u001b[0muWv\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmatmul\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0muW\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0md_output\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtranspose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-108-84c55347261a>\u001b[0m in \u001b[0;36mdecoder_RNN\u001b[0;34m(self, d_x, hiddens)\u001b[0m\n\u001b[1;32m     33\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     34\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mdecoder_RNN\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0md_x\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhiddens\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 35\u001b[0;31m         \u001b[0md_y\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhiddens2\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mRNN2\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0md_x\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhiddens\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     36\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0md_y\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhiddens2\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     37\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/py37cuda10/lib/python3.7/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    487\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    488\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 489\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    490\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    491\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/py37cuda10/lib/python3.7/site-packages/torch/nn/modules/rnn.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input, hx)\u001b[0m\n\u001b[1;32m    177\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mbatch_sizes\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    178\u001b[0m             result = _impl(input, hx, self._flat_weights, self.bias, self.num_layers,\n\u001b[0;32m--> 179\u001b[0;31m                            self.dropout, self.training, self.bidirectional, self.batch_first)\n\u001b[0m\u001b[1;32m    180\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    181\u001b[0m             result = _impl(input, batch_sizes, hx, self._flat_weights, self.bias,\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "ANSWERS = Inference(model_path=\"models/Model1\", question_path='sel_conversation/question.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ANSWERS "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
