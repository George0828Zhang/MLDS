{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import os\n",
    "# os.environ[\"CUDA_VISIBLE_DEVICES\"] =\"-1\"\n",
    "import torch\n",
    "import numpy as np\n",
    "import torch.nn as nn\n",
    "# import torch.nn.functional as F\n",
    "# import torchvision\n",
    "\n",
    "#from keras.preprocessing.sequence import pad_sequences\n",
    "#from keras.utils import to_categorical\n",
    "\n",
    "from tqdm import tqdm_notebook as tqdm\n",
    "\n",
    "import json\n",
    "import nltk\n",
    "from nltk import word_tokenize\n",
    "\n",
    "from torch.utils.data.dataloader import default_collate\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(device)\n",
    "DIC_word_index = json.load(open(\"vocab.json\", \"r\", encoding='utf-8'))\n",
    "DIC_index_word = {index:word for word, index in DIC_word_index.items()}\n",
    "word_vectors = np.load(\"wv_matrix100d.npy\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cpu\n"
     ]
    }
   ],
   "source": [
    "# if you don't have money, use this cell\n",
    "\n",
    "# device = torch.device(\"cpu\")\n",
    "# print(device)\n",
    "# DIC_word_index = json.load(open(\"vocab.json\", \"r\", encoding='utf-8'))\n",
    "# DIC_index_word = {index:word for word, index in DIC_word_index.items()}\n",
    "# word_vectors = np.load(\"wv_matrix100d.npy\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "sent_len = 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class InferenceAttModel(torch.nn.Module):\n",
    "    def __init__(self, latent_dim, seq_length, embed_dim, word_vectors):\n",
    "        super(InferenceAttModel, self).__init__()\n",
    "        \n",
    "        self.hidden_dim = latent_dim        \n",
    "        self.seq_length = seq_length\n",
    "        self.embed_dim = embed_dim\n",
    "        \n",
    "#         self.embedding = torch.nn.Embedding(len(word_vectors), embed_dim)\n",
    "#         self.embedding.load_state_dict({'weight': word_vectors})\n",
    "#         self.embedding.weight.requires_grad = False\n",
    "        self.embedding = torch.nn.Embedding.from_pretrained(torch.from_numpy(word_vectors), freeze=True)\n",
    "#        self.embedding = torch.nn.Embedding(len(word_vectors), embed_dim)\n",
    "    \n",
    "        self.soft = torch.nn.Softmax(-2)\n",
    "        \n",
    "        self.RNN1 = torch.nn.LSTM(self.embed_dim, latent_dim,  num_layers=2, batch_first = True)\n",
    "        self.RNN2 = torch.nn.LSTM(self.embed_dim, latent_dim,  num_layers=2, batch_first = True)\n",
    "        \n",
    "        self.Projection_layer = torch.nn.Sequential(\n",
    "            torch.nn.Linear(2 * latent_dim, len(word_vectors)),\n",
    "        )\n",
    "        \n",
    "        self.trainable_W = torch.nn.Sequential(\n",
    "            torch.nn.Linear(latent_dim ,  latent_dim),\n",
    "            torch.nn.Sigmoid(),\n",
    "            torch.nn.Dropout(p=0.2),\n",
    "        )\n",
    "    def encoder_RNN(self, e_x):\n",
    "        e_y, hiddens = self.RNN1(e_x)\n",
    "        \n",
    "        return e_y, hiddens\n",
    "    \n",
    "    def decoder_RNN(self, d_x, hiddens):\n",
    "        d_y, hiddens2 = self.RNN2(d_x, hiddens)\n",
    "        return d_y, hiddens2\n",
    "    \n",
    "    def forward(self, e_x, ):\n",
    "        e_x = self.embedding(e_x).float()\n",
    "        batch_size = e_x.shape[0]        \n",
    "        if(batch_size != 1):\n",
    "            print(batch_size)\n",
    "            print(\"Batch size > 1 is not available now. Batch size please give 1.\")\n",
    "            return\n",
    "        #print(sent.shape) torch.Size([2, 15, 6087])\n",
    "\n",
    "        e_output, hiddens = self.encoder_RNN(e_x)\n",
    "        \n",
    "        #attention\n",
    "        uW = self.trainable_W(e_output)\n",
    "        \n",
    "        first = True\n",
    "        first_index = DIC_word_index['<bos>']\n",
    "        first_input = torch.tensor([[first_index] * batch_size])\n",
    "        first_input = first_input.view(batch_size, 1, 1)\n",
    "        ans_seq = []\n",
    "        for i in range(self.seq_length):\n",
    "            #print(word.shape) torch.Size([2, 6087])\n",
    "            if(first):\n",
    "                word = first_input;\n",
    "                first = False\n",
    "                \n",
    "            #print(word.shape)\n",
    "            word = self.embedding(word.view(1, -1)).float();\n",
    "            #print(word.shape)\n",
    "            #one for each word, therefore d_output = d_state\n",
    "            d_output, hiddens = self.decoder_RNN(word, hiddens)\n",
    "            uWv = torch.matmul(uW, d_output.transpose(2,1))\n",
    "            #print(uWv.shape) torch.Size([2, 80, 1])\n",
    "            alpha = self.soft(uWv)\n",
    "            #print(uWv)\n",
    "            #print(alpha)\n",
    "            #input(\"\")\n",
    "            new_context = e_output.transpose(2,1) @ alpha;\n",
    "            #print(e_output.shape) torch.Size([2, 80, 256])\n",
    "            #print(alpha.shape) torch.Size([2, 80, 1])\n",
    "            \n",
    "            #print(d_output.shape) torch.Size([2, 1, 256])\n",
    "            #print(new_context.shape) torch.Size([2, 256, 1])          \n",
    "            pre = torch.cat((d_output, new_context.transpose(2,1)), 2)\n",
    "            ans_logits = self.Projection_layer(pre)\n",
    "            ans_index = np.argmax(ans_logits.cpu().detach().numpy())\n",
    "            ans = DIC_index_word[ans_index]\n",
    "            ans_seq.append(ans)\n",
    "            word = torch.tensor([ans_index])\n",
    "\n",
    "        #logits = torch.stack(logits, 1)\n",
    "        return ans_seq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#\n",
    "def Inference(model_path, question_path):\n",
    "    def pad_post_zero(a, length):\n",
    "        ret = []\n",
    "        for _list in a:\n",
    "            if(len(_list) < length):\n",
    "                for ct in range(len(_list),length,1):\n",
    "                    _list.append(DIC_word_index[\"<pad>\"])\n",
    "            if(len(_list) > length):\n",
    "                _list = _list[:length]\n",
    "\n",
    "            ret.append(_list)\n",
    "        return ret\n",
    "    \n",
    "    latent_dim = 1024\n",
    "    embed_dim = word_vectors.shape[1]\n",
    "    \n",
    "    model = InferenceAttModel(latent_dim, sent_len, embed_dim, word_vectors)\n",
    "    #saved_model = torch.load(\"models/Model92\")\n",
    "    saved_model = torch.load(model_path)\n",
    "    model.load_state_dict(saved_model['model'])\n",
    "    model = model.to(device)\n",
    "    \n",
    "    encode_x = []\n",
    "    #with open('sel_conversation/question.txt', 'r') as f:\n",
    "    with open(question_path, 'r') as f:\n",
    "        for l in f:\n",
    "            l = l.split()\n",
    "            tmp = []\n",
    "            for word in l:\n",
    "                try:\n",
    "                    index = DIC_word_index[word]\n",
    "                except KeyError:\n",
    "                    index = DIC_word_index['<unk>']\n",
    "                tmp.append(index)\n",
    "            encode_x.append([DIC_word_index['<bos>']] + tmp + [DIC_word_index['<eos>']])        \n",
    "    encode_x = pad_post_zero(encode_x, sent_len)\n",
    "    \n",
    "    \n",
    "    ANSWERS = list()\n",
    "    for index in range(5):\n",
    "        e_x = torch.tensor([encode_x[index]]).long().to(device)\n",
    "        ans = model.forward(e_x)\n",
    "        ANSWERS.append(ans)\n",
    "    \n",
    "    return(ANSWERS)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "ANSWERS = Inference(model_path=\"models/Model92\", question_path='sel_conversation/question.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['我們', '選', '了', '一個', '很'],\n",
       " ['來', '紀念', '退伍軍人', '節', '的'],\n",
       " ['今天', '早上', '<eos>', '會', '有'],\n",
       " ['就是', '保護', '我們', '了', '<eos>'],\n",
       " ['保護', '和', '權利', '而已', '<eos>']]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ANSWERS "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
