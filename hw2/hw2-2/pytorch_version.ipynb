{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import os\n",
    "#os.environ[\"CUDA_VISIBLE_DEVICES\"] =\"-1\"\n",
    "import torch\n",
    "import numpy as np\n",
    "import torch.nn as nn\n",
    "# import torch.nn.functional as F\n",
    "# import torchvision\n",
    "\n",
    "#from keras.preprocessing.sequence import pad_sequences\n",
    "#from keras.utils import to_categorical\n",
    "\n",
    "from tqdm import tqdm_notebook as tqdm\n",
    "\n",
    "import json\n",
    "import nltk\n",
    "from nltk import word_tokenize\n",
    "\n",
    "from torch.utils.data.dataloader import default_collate\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(device)\n",
    "DIC_word_index = json.load(open(\"vocab.json\", \"r\", encoding='utf-8'))\n",
    "DIC_index_word = {index:word for word, index in DIC_word_index.items()}\n",
    "word_vectors = np.load(\"wv_matrix100d.npy\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DIC_index_word[97]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def seq2sent(seq):\n",
    "    return [DIC_index_word[i] for i in seq]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def to_words(ys):       \n",
    "    tokens = []\n",
    "    #print(ys)\n",
    "    #input(\"\")\n",
    "    for catagorical_word in ys:\n",
    "        index = np.argmax(catagorical_word.cpu().detach().numpy())\n",
    "        #print(index)\n",
    "        if index in DIC_index_word:\n",
    "            tokens.append(DIC_index_word[index])\n",
    "        else:\n",
    "            tokens.append('<unk>');\n",
    "    return tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _run_iter(batch, training, model, loss_function):\n",
    "    with torch.no_grad():\n",
    "        e_x = batch['encoder_x'].long().to(device)\n",
    "        d_x = batch['decoder_x'].long().to(device)\n",
    "    output = model.forward(e_x, d_x, 0.5)\n",
    "    #print(output.shape)\n",
    "    #print(batch['decoder_y'].shape)\n",
    "    #input(\"\")\n",
    "    #print(output)\n",
    "    #input(\"\")\n",
    "    loss = loss_function(output.view(-1, len(word_vectors)), batch['decoder_y'].view(-1).long().to(device))\n",
    "    return output, loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _run_epoch(dataloader, training, model, optimizer, loss_function):\n",
    "    model.train(training)\n",
    "    if training:\n",
    "        iter_in_epoch = min(len(dataloader), 1000000)\n",
    "        description = 'train'\n",
    "    else:\n",
    "        iter_in_epoch = len(dataloader)\n",
    "        description = 'test'\n",
    "    grad_accumulate_steps = 1\n",
    "    trange = tqdm(enumerate(dataloader), total=iter_in_epoch, desc=description)\n",
    "    loss = 0\n",
    "    for i, batch in trange:   \n",
    "        if training and i >= iter_in_epoch:\n",
    "            break\n",
    "\n",
    "        if training:\n",
    "            #print(\"batch:{}\".format(batch))\n",
    "            #print(batch['context'].dtype)\n",
    "            optimizer.zero_grad()\n",
    "            output, batch_loss = _run_iter(batch, training, model, loss_function)            \n",
    "            \n",
    "            batch_loss /= grad_accumulate_steps\n",
    "            \n",
    "            if i % grad_accumulate_steps == 0:\n",
    "                optimizer.zero_grad()\n",
    "            \n",
    "            batch_loss.backward()\n",
    "\n",
    "            if (i + 1) % grad_accumulate_steps == 0:\n",
    "                optimizer.step()\n",
    "            if((i+1) % 2000 == 0):\n",
    "                print([DIC_index_word[i.item()] for i in batch['decoder_y'][0].cpu().detach()])\n",
    "                print(to_words(output[0]))\n",
    "                print(batch_loss)\n",
    "        else:\n",
    "            with torch.no_grad():\n",
    "                output, batch_loss = _run_iter(batch, training, model, loss_function)\n",
    "                if((i+1) % 2000 == 0):\n",
    "                    print([DIC_index_word[i.item()] for i in batch['decoder_y'][0].cpu().detach()])\n",
    "                    print(to_words(output[0]))\n",
    "                \n",
    "        loss += batch_loss.item()\n",
    "\n",
    "    loss /= iter_in_epoch\n",
    "    print('loss=%f\\n' % loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pad_post_zero(a, length):\n",
    "    ret = []\n",
    "    for _list in a:\n",
    "        if(len(_list) < length):\n",
    "            for ct in range(len(_list),length,1):\n",
    "                _list.append(DIC_word_index[\"<pad>\"])\n",
    "        if(len(_list) > length):\n",
    "            _list = _list[:length]\n",
    "            \n",
    "        ret.append(_list)\n",
    "    return ret"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def to_categorical(y, num_classes=None, dtype='float32'):\n",
    "    \"\"\"Converts a class vector (integers) to binary class matrix.\n",
    "    E.g. for use with categorical_crossentropy.\n",
    "    # Arguments\n",
    "        y: class vector to be converted into a matrix\n",
    "            (integers from 0 to num_classes).\n",
    "        num_classes: total number of classes.\n",
    "        dtype: The data type expected by the input, as a string\n",
    "            (`float32`, `float64`, `int32`...)\n",
    "    # Returns\n",
    "        A binary matrix representation of the input. The classes axis\n",
    "        is placed last.\n",
    "    # Example\n",
    "    ```python\n",
    "    # Consider an array of 5 labels out of a set of 3 classes {0, 1, 2}:\n",
    "    > labels\n",
    "    array([0, 2, 1, 2, 0])\n",
    "    # `to_categorical` converts this into a matrix with as many\n",
    "    # columns as there are classes. The number of rows\n",
    "    # stays the same.\n",
    "    > to_categorical(labels)\n",
    "    array([[ 1.,  0.,  0.],\n",
    "           [ 0.,  0.,  1.],\n",
    "           [ 0.,  1.,  0.],\n",
    "           [ 0.,  0.,  1.],\n",
    "           [ 1.,  0.,  0.]], dtype=float32)\n",
    "    ```\n",
    "    \"\"\"\n",
    "\n",
    "    y = np.array(y, dtype='int')\n",
    "    input_shape = y.shape\n",
    "    if input_shape and input_shape[-1] == 1 and len(input_shape) > 1:\n",
    "        input_shape = tuple(input_shape[:-1])\n",
    "    y = y.ravel()\n",
    "    if not num_classes:\n",
    "        num_classes = np.max(y) + 1\n",
    "    n = y.shape[0]\n",
    "    categorical = np.zeros((n, num_classes), dtype=dtype)\n",
    "    categorical[np.arange(n), y] = 1\n",
    "    output_shape = input_shape + (num_classes,)\n",
    "    categorical = np.reshape(categorical, output_shape)\n",
    "    return categorical\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def my_collate(datas):\n",
    "    batch = {}\n",
    "    # collate lists\n",
    "    batch['decoder_x'] = torch.tensor([data['decoder_x'] for data in datas])\n",
    "    batch['decoder_y'] = torch.tensor([data['decoder_y'] for data in datas])\n",
    "    batch['encoder_x'] = torch.tensor([data['encoder_x'] for data in datas])\n",
    "    return batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sent_len = 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "encode_x = []\n",
    "with open('sel_conversation/question.txt', 'r') as f:\n",
    "    for l in f:\n",
    "        l = l.split()\n",
    "        tmp = []\n",
    "        for word in l:\n",
    "            try:\n",
    "                index = DIC_word_index[word]\n",
    "            except KeyError:\n",
    "                index = DIC_word_index['<unk>']\n",
    "            tmp.append(index)\n",
    "        encode_x.append([DIC_word_index['<bos>']] + tmp + [DIC_word_index['<eos>']])        \n",
    "encode_x = pad_post_zero(encode_x, sent_len)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(seq2sent(encode_x[73]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "decode_x = []\n",
    "decode_y = []\n",
    "with open('sel_conversation/answer.txt', 'r') as f:\n",
    "    for l in f:\n",
    "        l = l.split()\n",
    "        tmp = []\n",
    "        for word in l:\n",
    "            try:\n",
    "                index = DIC_word_index[word]\n",
    "            except KeyError:\n",
    "                index = DIC_word_index['<unk>']\n",
    "            tmp.append(index)\n",
    "        decode_x.append([DIC_word_index['<bos>']] + tmp)    \n",
    "        decode_y.append( tmp + [DIC_word_index['<eos>']])\n",
    "    decode_x = pad_post_zero(decode_x, sent_len)\n",
    "    decode_y = pad_post_zero(decode_y, sent_len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(seq2sent(decode_x[68]))\n",
    "print(seq2sent(decode_y[68]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "datas = []\n",
    "for a,b,c in zip(decode_x, decode_y , encode_x):\n",
    "    data = {}\n",
    "\n",
    "    data['decoder_x'] = a\n",
    "    data['decoder_y'] = b;\n",
    "    data['encoder_x'] = c;\n",
    "    \n",
    "    datas.append(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SimpleRNN(torch.nn.Module):\n",
    "    def __init__(self, latent_dim, seq_length, embed_dim, word_vectors):\n",
    "        super(SimpleRNN, self).__init__()\n",
    "        \n",
    "        self.hidden_dim = latent_dim        \n",
    "        self.seq_length = seq_length\n",
    "        self.embed_dim = embed_dim\n",
    "        \n",
    "#         self.embedding = torch.nn.Embedding(len(word_vectors), embed_dim)\n",
    "#         self.embedding.load_state_dict({'weight': word_vectors})\n",
    "#         self.embedding.weight.requires_grad = False\n",
    "        self.embedding = torch.nn.Embedding.from_pretrained(torch.from_numpy(word_vectors),freeze=True)\n",
    "#        self.embedding = torch.nn.Embedding(len(word_vectors), embed_dim)\n",
    "    \n",
    "        self.GRU1 = torch.nn.GRU(self.embed_dim, latent_dim,  num_layers=1, batch_first = True)\n",
    "        self.GRU2 = torch.nn.GRU(self.embed_dim, latent_dim,  num_layers=1, batch_first = True)\n",
    "        \n",
    "        self.Projection_layer = torch.nn.Sequential(\n",
    "            torch.nn.Linear(latent_dim, len(word_vectors)),\n",
    "        )\n",
    "    def encoder_GRU(self, e_x):\n",
    "        e_y, hiddens = self.GRU1(e_x)\n",
    "        \n",
    "        return e_y, hiddens\n",
    "    \n",
    "    def decoder_GRU(self, d_x, hiddens):\n",
    "        d_y, hiddens2 = self.GRU2(d_x, hiddens)\n",
    "        return d_y, hiddens2\n",
    "    \n",
    "    def forward(self, e_x, d_x):\n",
    "        e_x = self.embedding(e_x).float()\n",
    "        \n",
    "        d_x = self.embedding(d_x).float()\n",
    "        \n",
    "        \n",
    "        e_y, hiddens = self.encoder_GRU(e_x)\n",
    "        \n",
    "        first = True\n",
    "        \n",
    "#         for word in d_x.transpose(1,0):\n",
    "#             word = word.view(word.shape[0], 1, -1)\n",
    "#             d_y, hiddens = self.decoder_GRU(word, hiddens)\n",
    "            \n",
    "# #             print(d_y.shape)\n",
    "#             if(first):\n",
    "#                 first = False;\n",
    "#                 last_layer_input = d_y\n",
    "#             else:\n",
    "#                 last_layer_input = torch.cat((last_layer_input, d_y), 1)\n",
    "#         #print(last_layer_input.shape)\n",
    "#         output = self.Projection_layer(last_layer_input)\n",
    "        for word in d_x.transpose(1,0):\n",
    "            word = word.view(word.shape[0], 1, -1)\n",
    "            d_y, hiddens = self.decoder_GRU(word, hiddens)\n",
    "            \n",
    "            if(first):\n",
    "                first = False;\n",
    "                last_layer_input = self.Projection_layer(d_y)\n",
    "            else:\n",
    "                last_layer_input = torch.cat((last_layer_input, self.Projection_layer(d_y)), 1)\n",
    "\n",
    "        output = last_layer_input\n",
    "        #print(output)\n",
    "        \n",
    "        return output\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AttModel(torch.nn.Module):\n",
    "    def __init__(self, latent_dim, seq_length, embed_dim, word_vectors):\n",
    "        super(AttModel, self).__init__()\n",
    "        \n",
    "        self.hidden_dim = latent_dim        \n",
    "        self.seq_length = seq_length\n",
    "        self.embed_dim = embed_dim\n",
    "        \n",
    "#         self.embedding = torch.nn.Embedding(len(word_vectors), embed_dim)\n",
    "#         self.embedding.load_state_dict({'weight': word_vectors})\n",
    "#         self.embedding.weight.requires_grad = False\n",
    "        self.embedding = torch.nn.Embedding.from_pretrained(torch.from_numpy(word_vectors), freeze=True)\n",
    "#        self.embedding = torch.nn.Embedding(len(word_vectors), embed_dim)\n",
    "    \n",
    "        self.soft = torch.nn.Softmax(-2)\n",
    "        \n",
    "        self.RNN1 = torch.nn.LSTM(self.embed_dim, latent_dim,  num_layers=2, batch_first = True)\n",
    "        self.RNN2 = torch.nn.LSTM(self.embed_dim, latent_dim,  num_layers=2, batch_first = True)\n",
    "        \n",
    "        self.Projection_layer = torch.nn.Sequential(\n",
    "            torch.nn.Linear(2 * latent_dim, len(word_vectors)),\n",
    "        )\n",
    "        \n",
    "        self.trainable_W = torch.nn.Sequential(\n",
    "            torch.nn.Linear(latent_dim ,  latent_dim),\n",
    "            torch.nn.Sigmoid(),\n",
    "            torch.nn.Dropout(p=0.2),\n",
    "        )\n",
    "    def encoder_RNN(self, e_x):\n",
    "        e_y, hiddens = self.RNN1(e_x)\n",
    "        \n",
    "        return e_y, hiddens\n",
    "    \n",
    "    def decoder_RNN(self, d_x, hiddens):\n",
    "        d_y, hiddens2 = self.RNN2(d_x, hiddens)\n",
    "        return d_y, hiddens2\n",
    "    \n",
    "    def forward(self, e_x, d_x, probs):\n",
    "        e_x = self.embedding(e_x).float()\n",
    "        \n",
    "        d_x = self.embedding(d_x).float()\n",
    "\n",
    "        #print(sent.shape) torch.Size([2, 15, 6087])\n",
    "\n",
    "        e_output, hiddens = self.encoder_RNN(e_x)\n",
    "        \n",
    "        #attention\n",
    "        uW = self.trainable_W(e_output)\n",
    "        \n",
    "        first = True\n",
    "        pre = None\n",
    "        for word in d_x.transpose(1,0):\n",
    "            #print(word.shape) torch.Size([2, 6087])\n",
    "            word = torch.unsqueeze(word , 1)\n",
    "            #torch.Size([128, 1, 100])\n",
    "            \n",
    "            #sample a word here\n",
    "            if (not first) and np.random.rand() < probs:\n",
    "                wordprob = self.Projection_layer(pre)\n",
    "                #print(wordprob.shape)torch.Size([128, 1, 46801])\n",
    "                ans_indices = torch.argmax(wordprob, dim=-1, keepdim=False)\n",
    "                word = self.embedding(ans_indices).float()\n",
    "                #torch.Size([128, 1, 100])\n",
    "            \n",
    "\n",
    "            #print(word.shape)\n",
    "            #one for each word, therefore d_output = d_state\n",
    "            d_output, hiddens = self.decoder_RNN(word, hiddens)\n",
    "            uWv = torch.matmul(uW, d_output.transpose(2,1))\n",
    "            #print(uWv.shape) torch.Size([2, 80, 1])\n",
    "            alpha = self.soft(uWv)\n",
    "            #print(uWv)\n",
    "            #print(alpha)\n",
    "            #input(\"\")\n",
    "            new_context = e_output.transpose(2,1) @ alpha;\n",
    "            #print(e_output.shape) torch.Size([2, 80, 256])\n",
    "            #print(alpha.shape) torch.Size([2, 80, 1])\n",
    "            \n",
    "            #print(d_output.shape) torch.Size([2, 1, 256])\n",
    "            #print(new_context.shape) torch.Size([2, 256, 1])          \n",
    "            pre = torch.cat((d_output, new_context.transpose(2,1)), 2)\n",
    "            if(first):\n",
    "                first = 0;\n",
    "                last_layer_input = pre\n",
    "            else:\n",
    "                last_layer_input = torch.cat((last_layer_input, pre), 1)\n",
    "            #print(last_layer_input.shape) #torch.Size([2, 1, 512])\n",
    "            \n",
    "        logits = self.Projection_layer(last_layer_input)\n",
    "\n",
    "        #logits = torch.stack(logits, 1)\n",
    "        return logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#hyperparameters\n",
    "lr = 1e-3  # learning rate\n",
    "batch_size = 128\n",
    "latent_dim = 1024\n",
    "iter_in_epoch = 500\n",
    "embed_dim = word_vectors.shape[1]\n",
    "print(word_vectors.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#model = SimpleRNN(latent_dim, sent_len, embed_dim, word_vectors)\n",
    "model = AttModel(latent_dim, sent_len, embed_dim, word_vectors)\n",
    "\n",
    "model = model.to(device)\n",
    "optimizer = torch.optim.Adam(model.parameters(),lr=lr)\n",
    "loss_function = torch.nn.CrossEntropyLoss(ignore_index=DIC_word_index['<pad>'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "epoch = 0;\n",
    "max_epochs = 100  # how many epochs to train for\n",
    "while epoch < max_epochs:\n",
    "    # train and evaluate train score\n",
    "    print('training %i' % epoch)\n",
    "\n",
    "    # train epoch\n",
    "    dataloader = torch.utils.data.DataLoader(datas, batch_size = batch_size, shuffle = True, collate_fn = my_collate)\n",
    "    log_train = _run_epoch(dataloader, True, model, optimizer, loss_function)\n",
    "\n",
    "    # test epoch\n",
    "    \"\"\"\n",
    "    print('evaluating %i' % epoch)\n",
    "    dataloader = torch.utils.data.DataLoader(valid_datas, batch_size = batch_size, collate_fn=my_collate)\n",
    "    log_valid = _run_epoch(dataloader, False, model, optimizer, loss_function)\n",
    "    \"\"\"\n",
    "    \n",
    "    epoch += 1\n",
    "    torch.save({\n",
    "        'epoch': epoch + 1,\n",
    "        'model': model.state_dict(),\n",
    "        'optimizer': optimizer.state_dict()\n",
    "    }, \"./models/Model\" + str(epoch))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(DIC_word_index['<pad>'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "model = AttModel(latent_dim, sent_len, embed_dim, word_vectors)\n",
    "# saved_model = torch.load(\"models/Model56\")\n",
    "# model.load_state_dict(saved_model['model'])\n",
    "\n",
    "model = model.to(device)\n",
    "optimizer = torch.optim.Adam(model.parameters(),lr=lr)\n",
    "# optimizer.load_state_dict(saved_model['optimizer'])\n",
    "loss_function = torch.nn.CrossEntropyLoss(ignore_index=DIC_word_index['<pad>'])\n",
    "\n",
    "# epoch = saved_model['epoch']\n",
    "max_epochs = 100  # how many epochs to train for\n",
    "while epoch < max_epochs:\n",
    "    # train and evaluate train score\n",
    "    print('training %i' % epoch)\n",
    "\n",
    "    # train epoch\n",
    "    dataloader = torch.utils.data.DataLoader(datas, batch_size = batch_size, shuffle = True, collate_fn = my_collate)\n",
    "    log_train = _run_epoch(dataloader, True, model, optimizer, loss_function)\n",
    "\n",
    "    # test epoch\n",
    "    \"\"\"\n",
    "    print('evaluating %i' % epoch)\n",
    "    dataloader = torch.utils.data.DataLoader(valid_datas, batch_size = batch_size, collate_fn=my_collate)\n",
    "    log_valid = _run_epoch(dataloader, False, model, optimizer, loss_function)\n",
    "    \"\"\"\n",
    "    \n",
    "    epoch += 1\n",
    "    torch.save({\n",
    "        'epoch': epoch + 1,\n",
    "        'model': model.state_dict(),\n",
    "        'optimizer': optimizer.state_dict()\n",
    "    }, \"./models/Model\" + str(epoch))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
