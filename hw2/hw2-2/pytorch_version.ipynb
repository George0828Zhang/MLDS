{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import os\n",
    "#os.environ[\"CUDA_VISIBLE_DEVICES\"] =\"-1\"\n",
    "import torch\n",
    "import numpy as np\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "# import torchvision\n",
    "\n",
    "#from keras.preprocessing.sequence import pad_sequences\n",
    "#from keras.utils import to_categorical\n",
    "\n",
    "from tqdm import tqdm_notebook as tqdm\n",
    "\n",
    "import json\n",
    "import nltk\n",
    "from nltk import word_tokenize\n",
    "\n",
    "from torch.utils.data.dataloader import default_collate\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(device)\n",
    "DIC_word_index = json.load(open(\"vocab.json\", \"r\", encoding='utf-8'))\n",
    "DIC_index_word = {index:word for word, index in DIC_word_index.items()}\n",
    "word_vectors = np.load(\"wv_matrix100d.npy\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'別'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "DIC_index_word[97]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def seq2sent(seq):\n",
    "    return [DIC_index_word[i] for i in seq]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def to_words(ys):       \n",
    "    tokens = []\n",
    "    #print(ys)\n",
    "    #input(\"\")\n",
    "    for catagorical_word in ys:\n",
    "        index = np.argmax(catagorical_word.cpu().detach().numpy())\n",
    "        #print(index)\n",
    "        if index in DIC_index_word:\n",
    "            tokens.append(DIC_index_word[index])\n",
    "        else:\n",
    "            tokens.append('<unk>');\n",
    "    return tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _run_iter(batch, training, model, loss_function):\n",
    "    with torch.no_grad():\n",
    "        e_x = batch['encoder_x'].long().to(device)\n",
    "        d_x = batch['decoder_x'].long().to(device)\n",
    "    output = model.forward(e_x, d_x)\n",
    "    #print(output.shape)\n",
    "    #print(batch['decoder_y'].shape)\n",
    "    #input(\"\")\n",
    "    #print(output)\n",
    "    #input(\"\")\n",
    "    loss = loss_function(output.view(-1, len(word_vectors)), batch['decoder_y'].view(-1).long().to(device))\n",
    "    return output, loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _run_epoch(dataloader, training, model, optimizer, loss_function):\n",
    "    model.train(training)\n",
    "    if training:\n",
    "        iter_in_epoch = min(len(dataloader), 1000000)\n",
    "        description = 'train'\n",
    "    else:\n",
    "        iter_in_epoch = len(dataloader)\n",
    "        description = 'test'\n",
    "    grad_accumulate_steps = 1\n",
    "    trange = tqdm(enumerate(dataloader), total=iter_in_epoch, desc=description)\n",
    "    loss = 0\n",
    "    for i, batch in trange:   \n",
    "        if training and i >= iter_in_epoch:\n",
    "            break\n",
    "\n",
    "        if training:\n",
    "            #print(\"batch:{}\".format(batch))\n",
    "            #print(batch['context'].dtype)\n",
    "            optimizer.zero_grad()\n",
    "            output, batch_loss = _run_iter(batch, training, model, loss_function)            \n",
    "            \n",
    "            batch_loss /= grad_accumulate_steps\n",
    "            \n",
    "            if i % grad_accumulate_steps == 0:\n",
    "                optimizer.zero_grad()\n",
    "            \n",
    "            batch_loss.backward()\n",
    "\n",
    "            if (i + 1) % grad_accumulate_steps == 0:\n",
    "                optimizer.step()\n",
    "            if((i+1) % 2000 == 0):\n",
    "                print([DIC_index_word[i.item()] for i in batch['decoder_y'][0].cpu().detach()])\n",
    "                print(to_words(output[0]))\n",
    "                print(batch_loss)\n",
    "        else:\n",
    "            with torch.no_grad():\n",
    "                output, batch_loss = _run_iter(batch, training, model, loss_function)\n",
    "                if((i+1) % 2000 == 0):\n",
    "                    print([DIC_index_word[i.item()] for i in batch['decoder_y'][0].cpu().detach()])\n",
    "                    print(to_words(output[0]))\n",
    "                \n",
    "        loss += batch_loss.item()\n",
    "\n",
    "    loss /= iter_in_epoch\n",
    "    print('loss=%f\\n' % loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pad_post_zero(a, length):\n",
    "    ret = []\n",
    "    for _list in a:\n",
    "        if(len(_list) < length):\n",
    "            for ct in range(len(_list),length,1):\n",
    "                _list.append(DIC_word_index[\"<pad>\"])\n",
    "        if(len(_list) > length):\n",
    "            _list = _list[:length]\n",
    "            \n",
    "        ret.append(_list)\n",
    "    return ret"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def to_categorical(y, num_classes=None, dtype='float32'):\n",
    "    \"\"\"Converts a class vector (integers) to binary class matrix.\n",
    "    E.g. for use with categorical_crossentropy.\n",
    "    # Arguments\n",
    "        y: class vector to be converted into a matrix\n",
    "            (integers from 0 to num_classes).\n",
    "        num_classes: total number of classes.\n",
    "        dtype: The data type expected by the input, as a string\n",
    "            (`float32`, `float64`, `int32`...)\n",
    "    # Returns\n",
    "        A binary matrix representation of the input. The classes axis\n",
    "        is placed last.\n",
    "    # Example\n",
    "    ```python\n",
    "    # Consider an array of 5 labels out of a set of 3 classes {0, 1, 2}:\n",
    "    > labels\n",
    "    array([0, 2, 1, 2, 0])\n",
    "    # `to_categorical` converts this into a matrix with as many\n",
    "    # columns as there are classes. The number of rows\n",
    "    # stays the same.\n",
    "    > to_categorical(labels)\n",
    "    array([[ 1.,  0.,  0.],\n",
    "           [ 0.,  0.,  1.],\n",
    "           [ 0.,  1.,  0.],\n",
    "           [ 0.,  0.,  1.],\n",
    "           [ 1.,  0.,  0.]], dtype=float32)\n",
    "    ```\n",
    "    \"\"\"\n",
    "\n",
    "    y = np.array(y, dtype='int')\n",
    "    input_shape = y.shape\n",
    "    if input_shape and input_shape[-1] == 1 and len(input_shape) > 1:\n",
    "        input_shape = tuple(input_shape[:-1])\n",
    "    y = y.ravel()\n",
    "    if not num_classes:\n",
    "        num_classes = np.max(y) + 1\n",
    "    n = y.shape[0]\n",
    "    categorical = np.zeros((n, num_classes), dtype=dtype)\n",
    "    categorical[np.arange(n), y] = 1\n",
    "    output_shape = input_shape + (num_classes,)\n",
    "    categorical = np.reshape(categorical, output_shape)\n",
    "    return categorical\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def my_collate(datas):\n",
    "    batch = {}\n",
    "    # collate lists\n",
    "    batch['decoder_x'] = torch.tensor([data['decoder_x'] for data in datas])\n",
    "    batch['decoder_y'] = torch.tensor([data['decoder_y'] for data in datas])\n",
    "    batch['encoder_x'] = torch.tensor([data['encoder_x'] for data in datas])\n",
    "    return batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sent_len = 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "encode_x = []\n",
    "with open('sel_conversation/question.txt', 'r') as f:\n",
    "    for l in f:\n",
    "        l = l.split()\n",
    "        tmp = []\n",
    "        for word in l:\n",
    "            try:\n",
    "                index = DIC_word_index[word]\n",
    "            except KeyError:\n",
    "                index = DIC_word_index['<unk>']\n",
    "            tmp.append(index)\n",
    "        encode_x.append(tmp)        \n",
    "encode_x = pad_post_zero(encode_x, sent_len)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['導致', '四個', '州', '停電', '<pad>']\n"
     ]
    }
   ],
   "source": [
    "print(seq2sent(encode_x[73]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "decode_x = []\n",
    "decode_y = []\n",
    "with open('sel_conversation/answer.txt', 'r') as f:\n",
    "    for l in f:\n",
    "        l = l.split()\n",
    "        tmp = []\n",
    "        for word in l:\n",
    "            try:\n",
    "                index = DIC_word_index[word]\n",
    "            except KeyError:\n",
    "                index = DIC_word_index['<unk>']\n",
    "            tmp.append(index)\n",
    "        decode_x.append([DIC_word_index['<bos>']] + tmp)    \n",
    "        decode_y.append( tmp + [DIC_word_index['<eos>']])\n",
    "    decode_x = pad_post_zero(decode_x, sent_len)\n",
    "    decode_y = pad_post_zero(decode_y, sent_len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['<bos>', '在', '最新', '民調', '中']\n",
      "['在', '最新', '民調', '中', '<eos>']\n"
     ]
    }
   ],
   "source": [
    "print(seq2sent(decode_x[68]))\n",
    "print(seq2sent(decode_y[68]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "datas = []\n",
    "for a,b,c in zip(decode_x, decode_y , encode_x):\n",
    "    data = {}\n",
    "\n",
    "    data['decoder_x'] = a\n",
    "    data['decoder_y'] = b;\n",
    "    data['encoder_x'] = c;\n",
    "    datas.append(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(97841, 100)\n"
     ]
    }
   ],
   "source": [
    "#hyperparameters\n",
    "lr = 1e-3  # learning rate\n",
    "batch_size = 16\n",
    "latent_dim = 256\n",
    "iter_in_epoch = 500\n",
    "embed_dim = word_vectors.shape[1]\n",
    "print(word_vectors.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SimpleRNN(torch.nn.Module):\n",
    "    def __init__(self, latent_dim, seq_length, embed_dim, word_vectors):\n",
    "        super(SimpleRNN, self).__init__()\n",
    "        \n",
    "        self.hidden_dim = latent_dim        \n",
    "        self.seq_length = seq_length\n",
    "        self.embed_dim = embed_dim\n",
    "        \n",
    "#         self.embedding = torch.nn.Embedding(len(word_vectors), embed_dim)\n",
    "#         self.embedding.load_state_dict({'weight': word_vectors})\n",
    "#         self.embedding.weight.requires_grad = False\n",
    "#         self.embedding = torch.nn.Embedding.from_pretrained(torch.from_numpy(word_vectors),freeze=True)\n",
    "        self.embedding = torch.nn.Embedding(len(word_vectors), embed_dim)\n",
    "    \n",
    "        self.GRU1 = torch.nn.GRU(self.embed_dim, latent_dim,  num_layers=1, batch_first = True)\n",
    "        self.GRU2 = torch.nn.GRU(self.embed_dim, latent_dim,  num_layers=1, batch_first = True)\n",
    "        \n",
    "        self.Projection_layer = torch.nn.Sequential(\n",
    "            torch.nn.Linear(latent_dim, len(word_vectors)),\n",
    "            torch.nn.Softmax(-1),\n",
    "        )\n",
    "    def encoder_GRU(self, e_x):\n",
    "        e_y, hiddens = self.GRU1(e_x)\n",
    "        \n",
    "        return e_y, hiddens\n",
    "    \n",
    "    def decoder_GRU(self, d_x, hiddens):\n",
    "        d_y, hiddens2 = self.GRU2(d_x, hiddens)\n",
    "        return d_y, hiddens2\n",
    "    \n",
    "    def forward(self, e_x, d_x):\n",
    "        e_x = self.embedding(e_x).float()\n",
    "        \n",
    "        d_x = self.embedding(d_x).float()\n",
    "        \n",
    "        \n",
    "        e_y, hiddens = self.encoder_GRU(e_x)\n",
    "        \n",
    "        first = True\n",
    "        \n",
    "#         for word in d_x.transpose(1,0):\n",
    "#             word = word.view(word.shape[0], 1, -1)\n",
    "#             d_y, hiddens = self.decoder_GRU(word, hiddens)\n",
    "            \n",
    "# #             print(d_y.shape)\n",
    "#             if(first):\n",
    "#                 first = False;\n",
    "#                 last_layer_input = d_y\n",
    "#             else:\n",
    "#                 last_layer_input = torch.cat((last_layer_input, d_y), 1)\n",
    "#         #print(last_layer_input.shape)\n",
    "#         output = self.Projection_layer(last_layer_input)\n",
    "        for word in d_x.transpose(1,0):\n",
    "            word = word.view(word.shape[0], 1, -1)\n",
    "            d_y, hiddens = self.decoder_GRU(word, hiddens)\n",
    "            \n",
    "            if(first):\n",
    "                first = False;\n",
    "                last_layer_input = self.Projection_layer(d_y)\n",
    "            else:\n",
    "                last_layer_input = torch.cat((last_layer_input, self.Projection_layer(d_y)), 1)\n",
    "\n",
    "        output = last_layer_input\n",
    "        #print(output)\n",
    "        \n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = SimpleRNN(latent_dim, sent_len, embed_dim, word_vectors)\n",
    "\n",
    "model = model.to(device)\n",
    "optimizer = torch.optim.Adam(model.parameters(),lr=lr)\n",
    "loss_function = torch.nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training 0\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cfdcbe74bc3f44c2a3dc935c3f7386ed",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, description='train', max=55039, style=ProgressStyle(description_width='ini…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['什麼', '?', '<eos>', '<pad>', '<pad>']\n",
      "['我', '<eos>', '<eos>', '<pad>', '<pad>']\n",
      "tensor(11.2305, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "['好', '<eos>', '<pad>', '<pad>', '<pad>']\n",
      "['我', '<eos>', '<pad>', '<pad>', '<pad>']\n",
      "tensor(11.2037, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "['沒有', '太多', '的', '離開', '了']\n",
      "['我', '<eos>', '<eos>', '<eos>', '<eos>']\n",
      "tensor(11.2951, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "['好', '男人', ',', '成長', '起來']\n",
      "['我', '<eos>', '<eos>', '<eos>', '我']\n",
      "tensor(11.1875, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "['可是', '這樣', '要', '發現', '冰山']\n",
      "['我', '<eos>', '<eos>', '<eos>', '<eos>']\n",
      "tensor(11.2777, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "['我們', '是', '唯一', '的', '贏家']\n",
      "['我', '<eos>', '我', '<eos>', '<eos>']\n",
      "tensor(11.3551, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "['您', '不', '走', '我', '也']\n",
      "['我', '的', '<eos>', '<eos>', '的']\n",
      "tensor(11.3161, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "['對不起', '。', '<eos>', '<pad>', '<pad>']\n",
      "['我', '<eos>', '<eos>', '<pad>', '<pad>']\n",
      "tensor(11.2911, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "['你', '欺負', '咱', '不', '認識']\n",
      "['我', '的', '<eos>', '<eos>', '<eos>']\n",
      "tensor(11.3507, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "['此', '制度', '成功', '的', '案例']\n",
      "['我', '<eos>', '<eos>', '<eos>', '<eos>']\n",
      "tensor(11.2959, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "['告訴', '我', ',', '王子', '的']\n",
      "['我', '我', '的', '<eos>', '<eos>']\n",
      "tensor(11.2162, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "['這次', '調查', '你', '負責', '全局']\n",
      "['我', '我', '<eos>', '的', '<eos>']\n",
      "tensor(11.2462, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "['所有', '的', '實驗', '都', '表明']\n",
      "['我', '的', '<eos>', '<eos>', '<eos>']\n",
      "tensor(11.2536, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "['不', ',', '他', '不會', '<eos>']\n",
      "['我', '<eos>', '<eos>', '的', '<eos>']\n",
      "tensor(11.2287, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "['你好', '<eos>', '<pad>', '<pad>', '<pad>']\n",
      "['我', '<eos>', '<pad>', '<pad>', '<pad>']\n",
      "tensor(11.2036, device='cuda:0', grad_fn=<DivBackward0>)\n"
     ]
    }
   ],
   "source": [
    "epoch = 0;\n",
    "max_epochs = 100  # how many epochs to train for\n",
    "while epoch < max_epochs:\n",
    "    # train and evaluate train score\n",
    "    print('training %i' % epoch)\n",
    "\n",
    "    # train epoch\n",
    "    dataloader = torch.utils.data.DataLoader(datas, batch_size = batch_size, shuffle = True, collate_fn = my_collate)\n",
    "    log_train = _run_epoch(dataloader, True, model, optimizer, loss_function)\n",
    "\n",
    "    # test epoch\n",
    "    \"\"\"\n",
    "    print('evaluating %i' % epoch)\n",
    "    dataloader = torch.utils.data.DataLoader(valid_datas, batch_size = batch_size, collate_fn=my_collate)\n",
    "    log_valid = _run_epoch(dataloader, False, model, optimizer, loss_function)\n",
    "    \"\"\"\n",
    "    epoch += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(DIC_word_index['<pad>'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
